#!/usr/bin/env python3
"""
Test Generation Modules

Created: 2025-06-16 with user permission
Purpose: Generate executable test cases for unit, integration, performance, and user acceptance testing

Intent: Implements specialized test generators for different test levels, creating executable
test code that validates system behavior without implementation knowledge. Each generator
focuses on a specific test level while maintaining independence from implementation details.
"""

import os
import re
import json
import textwrap
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import logging
from pathlib import Path

from test_agent_framework import (
    TestCase, TestScenario, TestSuite, TestableBehavior,
    ScenarioType, TestLevel, ValidationResult, InterfaceSpecification, InterfaceMethod
)

logger = logging.getLogger(__name__)


class TestFramework(Enum):
    """Supported test frameworks"""
    PYTEST = "pytest"
    UNITTEST = "unittest"
    NOSE = "nose"


class TestOutputFormat(Enum):
    """Test output formats"""
    PYTHON_FILE = "python_file"
    JSON_SPEC = "json_spec"
    YAML_SPEC = "yaml_spec"
    MARKDOWN_DOC = "markdown_doc"


@dataclass
class TestCodeTemplate:
    """Template for generating test code"""
    framework: TestFramework
    imports: List[str]
    class_template: str
    method_template: str
    setup_template: str
    teardown_template: str
    assertion_templates: Dict[str, str]


@dataclass
class GeneratedTestFile:
    """Generated test file with metadata"""
    file_path: str
    framework: TestFramework
    test_count: int
    test_levels: List[TestLevel]
    user_story_id: str
    content: str
    dependencies: List[str] = field(default_factory=list)


@dataclass
class MockSpecification:
    """Specification for generating mocks"""
    mock_name: str
    interface_name: str
    mock_methods: List[str]
    mock_behaviors: Dict[str, Any]
    configuration_options: Dict[str, Any] = field(default_factory=dict)


class BaseTestGenerator(ABC):
    """Base class for all test generators"""
    
    def __init__(self, framework: TestFramework = TestFramework.PYTEST):
        self.framework = framework
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.templates = self._load_templates()
    
    @abstractmethod
    def generate_tests(self, test_cases: List[TestCase], output_dir: str) -> List[GeneratedTestFile]:
        """Generate test files for given test cases"""
        pass
    
    @abstractmethod
    def get_test_level(self) -> TestLevel:
        """Get the test level this generator handles"""
        pass
    
    def _load_templates(self) -> TestCodeTemplate:
        """Load test code templates for the framework"""
        if self.framework == TestFramework.PYTEST:
            return self._load_pytest_templates()
        elif self.framework == TestFramework.UNITTEST:
            return self._load_unittest_templates()
        else:
            raise ValueError(f"Unsupported test framework: {self.framework}")
    
    def _load_pytest_templates(self) -> TestCodeTemplate:
        """Load pytest templates"""
        return TestCodeTemplate(
            framework=TestFramework.PYTEST,
            imports=[
                "import pytest",
                "from unittest.mock import Mock, patch, MagicMock",
                "import pandas as pd",
                "import numpy as np",
                "from pathlib import Path",
                "import tempfile",
                "import time"
            ],
            class_template=textwrap.dedent("""
                class Test{class_name}:
                    \"\"\"
                    Test class for {description}
                    
                    Generated by Test Agent from user story: {user_story_id}
                    Test Level: {test_level}
                    \"\"\"
                    
                {class_body}
            """).strip(),
            method_template=textwrap.dedent("""
                def test_{method_name}(self{method_params}):
                    \"\"\"
                    {test_description}
                    
                    Scenario: {scenario_type}
                    Expected Behavior: {expected_behavior}
                    \"\"\"
                {method_body}
            """).strip(),
            setup_template=textwrap.dedent("""
                @pytest.fixture(autouse=True)
                def setup_method(self):
                    \"\"\"Setup for each test method\"\"\"
                {setup_body}
                    yield
                {teardown_body}
            """).strip(),
            teardown_template="    # Teardown handled in setup_method fixture",
            assertion_templates={
                "success": "    assert result.success, f'Operation should succeed: {result.errors}'",
                "failure": "    assert not result.success, 'Operation should fail'",
                "contains": "    assert '{value}' in str(result), f'Result should contain {value}'",
                "time_limit": "    assert execution_time <= {limit}, f'Execution time {execution_time} exceeds limit {limit}'",
                "not_none": "    assert result is not None, 'Result should not be None'",
                "equals": "    assert result == {expected}, f'Expected {expected}, got {result}'",
                "type_check": "    assert isinstance(result, {type_name}), f'Result should be {type_name}, got {type(result)}'"
            }
        )
    
    def _load_unittest_templates(self) -> TestCodeTemplate:
        """Load unittest templates"""
        return TestCodeTemplate(
            framework=TestFramework.UNITTEST,
            imports=[
                "import unittest",
                "from unittest.mock import Mock, patch, MagicMock",
                "import pandas as pd",
                "import numpy as np",
                "from pathlib import Path",
                "import tempfile",
                "import time"
            ],
            class_template=textwrap.dedent("""
                class Test{class_name}(unittest.TestCase):
                    \"\"\"
                    Test class for {description}
                    
                    Generated by Test Agent from user story: {user_story_id}
                    Test Level: {test_level}
                    \"\"\"
                    
                {class_body}
            """).strip(),
            method_template=textwrap.dedent("""
                def test_{method_name}(self):
                    \"\"\"
                    {test_description}
                    
                    Scenario: {scenario_type}
                    Expected Behavior: {expected_behavior}
                    \"\"\"
                {method_body}
            """).strip(),
            setup_template=textwrap.dedent("""
                def setUp(self):
                    \"\"\"Setup for each test method\"\"\"
                {setup_body}
            """).strip(),
            teardown_template=textwrap.dedent("""
                def tearDown(self):
                    \"\"\"Teardown for each test method\"\"\"
                {teardown_body}
            """).strip(),
            assertion_templates={
                "success": "    self.assertTrue(result.success, f'Operation should succeed: {result.errors}')",
                "failure": "    self.assertFalse(result.success, 'Operation should fail')",
                "contains": "    self.assertIn('{value}', str(result), f'Result should contain {value}')",
                "time_limit": "    self.assertLessEqual(execution_time, {limit}, f'Execution time {execution_time} exceeds limit {limit}')",
                "not_none": "    self.assertIsNotNone(result, 'Result should not be None')",
                "equals": "    self.assertEqual(result, {expected}, f'Expected {expected}, got {result}')",
                "type_check": "    self.assertIsInstance(result, {type_name}, f'Result should be {type_name}, got {type(result)}')"
            }
        )
    
    def _generate_imports(self, additional_imports: List[str] = None) -> str:
        """Generate import statements"""
        imports = self.templates.imports.copy()
        if additional_imports:
            imports.extend(additional_imports)
        return "\n".join(imports)
    
    def _generate_class_name(self, base_name: str) -> str:
        """Generate appropriate class name"""
        # Clean and capitalize base name
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '', base_name)
        return ''.join(word.capitalize() for word in clean_name.split('_'))
    
    def _generate_method_name(self, test_case: TestCase) -> str:
        """Generate method name from test case"""
        # Clean test case name for method name
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', test_case.name.lower())
        return clean_name.replace('__', '_').strip('_')


class UnitTestGenerator(BaseTestGenerator):
    """Generates unit tests for component behavioral validation"""
    
    def get_test_level(self) -> TestLevel:
        return TestLevel.UNIT
    
    def generate_tests(self, test_cases: List[TestCase], output_dir: str) -> List[GeneratedTestFile]:
        """Generate unit test files"""
        self.logger.info(f"Generating unit tests for {len(test_cases)} test cases")
        
        # Group test cases by component/module
        grouped_cases = self._group_test_cases_by_component(test_cases)
        
        generated_files = []
        
        for component, cases in grouped_cases.items():
            file_content = self._generate_unit_test_file(component, cases)
            
            file_path = os.path.join(output_dir, f"test_unit_{component.lower()}.py")
            
            generated_file = GeneratedTestFile(
                file_path=file_path,
                framework=self.framework,
                test_count=len(cases),
                test_levels=[TestLevel.UNIT],
                user_story_id=cases[0].scenario.behavior.name.split('_')[0] if cases else "unknown",
                content=file_content,
                dependencies=self._extract_dependencies(cases)
            )
            
            generated_files.append(generated_file)
        
        self.logger.info(f"Generated {len(generated_files)} unit test files")
        return generated_files
    
    def _group_test_cases_by_component(self, test_cases: List[TestCase]) -> Dict[str, List[TestCase]]:
        """Group test cases by component being tested"""
        groups = {}
        
        for test_case in test_cases:
            if test_case.test_level == TestLevel.UNIT:
                # Extract component name from test case
                component = self._extract_component_name(test_case)
                if component not in groups:
                    groups[component] = []
                groups[component].append(test_case)
        
        return groups
    
    def _extract_component_name(self, test_case: TestCase) -> str:
        """Extract component name from test case"""
        # Try to extract from test case name or description
        name_parts = test_case.name.split('_')
        
        # Look for component patterns
        for part in name_parts:
            if any(keyword in part.lower() for keyword in ['validator', 'converter', 'generator', 'analyzer']):
                return part
        
        # Default to behavior name prefix
        behavior_parts = test_case.scenario.behavior.name.split('_')
        return behavior_parts[1] if len(behavior_parts) > 1 else "component"
    
    def _generate_unit_test_file(self, component: str, test_cases: List[TestCase]) -> str:
        """Generate complete unit test file"""
        class_name = self._generate_class_name(component)
        
        # Generate test methods
        test_methods = []
        setup_code_parts = []
        teardown_code_parts = []
        
        for test_case in test_cases:
            method_code = self._generate_unit_test_method(test_case)
            test_methods.append(method_code)
            
            # Collect setup/teardown code
            if test_case.setup_code.strip():
                setup_code_parts.append(f"    # Setup for {test_case.name}")
                setup_code_parts.append(self._indent_code(test_case.setup_code, 2))
            
            if test_case.teardown_code.strip():
                teardown_code_parts.append(f"    # Teardown for {test_case.name}")
                teardown_code_parts.append(self._indent_code(test_case.teardown_code, 2))
        
        # Generate setup method
        setup_method = self._generate_setup_method(setup_code_parts, teardown_code_parts)
        
        # Combine class body
        class_body = setup_method + "\n\n" + "\n\n".join(test_methods)
        
        # Generate complete class
        class_code = self.templates.class_template.format(
            class_name=class_name,
            description=f"{component} unit tests",
            user_story_id=test_cases[0].scenario.behavior.name.split('_')[0] if test_cases else "unknown",
            test_level="Unit",
            class_body=self._indent_code(class_body, 1)
        )
        
        # Add imports and file header
        file_content = self._generate_file_header(component, "unit") + "\n\n"
        file_content += self._generate_imports() + "\n\n"
        file_content += class_code
        
        # Add main block for unittest
        if self.framework == TestFramework.UNITTEST:
            file_content += "\n\nif __name__ == '__main__':\n    unittest.main()"
        
        return file_content
    
    def _generate_unit_test_method(self, test_case: TestCase) -> str:
        """Generate individual unit test method"""
        method_name = self._generate_method_name(test_case)
        
        # Generate method body
        method_body_parts = []
        
        # Add test setup
        method_body_parts.append("    # Test setup")
        method_body_parts.append("    test_data = self._create_test_data()")
        
        # Add mock setup
        if test_case.mock_dependencies:
            method_body_parts.append("    # Mock setup")
            for mock_dep in test_case.mock_dependencies:
                mock_var = mock_dep.lower().replace(' ', '_')
                method_body_parts.append(f"    {mock_var} = Mock()")
        
        # Add test execution
        method_body_parts.append("    # Test execution")
        method_body_parts.append(f"    # Execute: {test_case.scenario.expected_behavior}")
        
        if test_case.scenario.scenario_type == ScenarioType.PERFORMANCE:
            method_body_parts.append("    start_time = time.time()")
            method_body_parts.append("    result = self._execute_test_operation(test_data)")
            method_body_parts.append("    execution_time = time.time() - start_time")
        else:
            method_body_parts.append("    result = self._execute_test_operation(test_data)")
        
        # Add assertions
        method_body_parts.append("    # Test assertions")
        assertions = self._generate_unit_test_assertions(test_case)
        method_body_parts.extend(assertions)
        
        method_body = "\n".join(method_body_parts)
        
        # Generate method parameters
        method_params = ""
        if self.framework == TestFramework.PYTEST:
            # Add fixture parameters if needed
            if test_case.mock_dependencies:
                method_params = ", mock_dependencies"
        
        return self.templates.method_template.format(
            method_name=method_name,
            method_params=method_params,
            test_description=test_case.description,
            scenario_type=test_case.scenario.scenario_type.value,
            expected_behavior=test_case.scenario.expected_behavior,
            method_body=method_body
        )
    
    def _generate_unit_test_assertions(self, test_case: TestCase) -> List[str]:
        """Generate assertions for unit test"""
        assertions = []
        
        # Basic not-none assertion
        assertions.append(self.templates.assertion_templates["not_none"])
        
        # Scenario-specific assertions
        if test_case.scenario.scenario_type == ScenarioType.HAPPY_PATH:
            assertions.append(self.templates.assertion_templates["success"])
        elif test_case.scenario.scenario_type == ScenarioType.ERROR:
            assertions.append(self.templates.assertion_templates["failure"])
            # Add specific error assertions
            for expected_error in test_case.scenario.expected_errors:
                assertion = self.templates.assertion_templates["contains"].format(
                    value=expected_error
                )
                assertions.append(assertion)
        elif test_case.scenario.scenario_type == ScenarioType.PERFORMANCE:
            # Add performance assertions
            for perf_req in test_case.scenario.performance_requirements:
                if "minutes" in perf_req.lower():
                    time_limit = self._extract_time_limit(perf_req, "minutes") * 60
                elif "seconds" in perf_req.lower():
                    time_limit = self._extract_time_limit(perf_req, "seconds")
                else:
                    time_limit = 120  # Default 2 minutes
                
                assertion = self.templates.assertion_templates["time_limit"].format(
                    limit=time_limit
                )
                assertions.append(assertion)
        
        # Add custom assertions from test case
        for assertion in test_case.assertions:
            assertions.append(f"    {assertion}")
        
        return assertions
    
    def _generate_setup_method(self, setup_parts: List[str], teardown_parts: List[str]) -> str:
        """Generate setup method for the test class"""
        setup_body = "\n".join(setup_parts) if setup_parts else "    pass"
        teardown_body = "\n".join(teardown_parts) if teardown_parts else "    pass"
        
        if self.framework == TestFramework.PYTEST:
            return self.templates.setup_template.format(
                setup_body=setup_body,
                teardown_body=teardown_body
            )
        else:
            setup_method = self.templates.setup_template.format(setup_body=setup_body)
            teardown_method = self.templates.teardown_template.format(teardown_body=teardown_body)
            return setup_method + "\n\n" + teardown_method


class IntegrationTestGenerator(BaseTestGenerator):
    """Generates integration tests for workflow and data flow validation"""
    
    def get_test_level(self) -> TestLevel:
        return TestLevel.INTEGRATION
    
    def generate_tests(self, test_cases: List[TestCase], output_dir: str) -> List[GeneratedTestFile]:
        """Generate integration test files"""
        self.logger.info(f"Generating integration tests for {len(test_cases)} test cases")
        
        # Group test cases by workflow
        grouped_cases = self._group_test_cases_by_workflow(test_cases)
        
        generated_files = []
        
        for workflow, cases in grouped_cases.items():
            file_content = self._generate_integration_test_file(workflow, cases)
            
            file_path = os.path.join(output_dir, f"test_integration_{workflow.lower()}.py")
            
            generated_file = GeneratedTestFile(
                file_path=file_path,
                framework=self.framework,
                test_count=len(cases),
                test_levels=[TestLevel.INTEGRATION],
                user_story_id=cases[0].scenario.behavior.name.split('_')[0] if cases else "unknown",
                content=file_content,
                dependencies=self._extract_dependencies(cases)
            )
            
            generated_files.append(generated_file)
        
        self.logger.info(f"Generated {len(generated_files)} integration test files")
        return generated_files
    
    def _group_test_cases_by_workflow(self, test_cases: List[TestCase]) -> Dict[str, List[TestCase]]:
        """Group test cases by workflow"""
        groups = {}
        
        for test_case in test_cases:
            if test_case.test_level == TestLevel.INTEGRATION:
                # Extract workflow name from test case
                workflow = self._extract_workflow_name(test_case)
                if workflow not in groups:
                    groups[workflow] = []
                groups[workflow].append(test_case)
        
        return groups
    
    def _extract_workflow_name(self, test_case: TestCase) -> str:
        """Extract workflow name from test case"""
        behavior_name = test_case.scenario.behavior.name
        
        # Common workflow patterns
        if "conversion" in behavior_name.lower():
            return "dataset_conversion"
        elif "validation" in behavior_name.lower():
            return "dataset_validation"
        elif "generation" in behavior_name.lower():
            return "report_generation"
        elif "workflow" in behavior_name.lower():
            return "end_to_end_workflow"
        else:
            return "general_integration"
    
    def _generate_integration_test_file(self, workflow: str, test_cases: List[TestCase]) -> str:
        """Generate complete integration test file"""
        class_name = self._generate_class_name(f"{workflow}_integration")
        
        # Generate test methods with workflow context
        test_methods = []
        
        for test_case in test_cases:
            method_code = self._generate_integration_test_method(test_case)
            test_methods.append(method_code)
        
        # Generate workflow setup method
        workflow_setup = self._generate_workflow_setup_method(workflow, test_cases)
        
        # Combine class body
        class_body = workflow_setup + "\n\n" + "\n\n".join(test_methods)
        
        # Generate complete class
        class_code = self.templates.class_template.format(
            class_name=class_name,
            description=f"{workflow} integration tests",
            user_story_id=test_cases[0].scenario.behavior.name.split('_')[0] if test_cases else "unknown",
            test_level="Integration",
            class_body=self._indent_code(class_body, 1)
        )
        
        # Add imports and file header
        file_content = self._generate_file_header(workflow, "integration") + "\n\n"
        file_content += self._generate_imports(["import subprocess", "import shutil"]) + "\n\n"
        file_content += class_code
        
        if self.framework == TestFramework.UNITTEST:
            file_content += "\n\nif __name__ == '__main__':\n    unittest.main()"
        
        return file_content
    
    def _generate_integration_test_method(self, test_case: TestCase) -> str:
        """Generate integration test method"""
        method_name = self._generate_method_name(test_case)
        
        # Generate method body for integration test
        method_body_parts = []
        
        # Integration test setup
        method_body_parts.append("    # Integration test setup")
        method_body_parts.append("    workflow_context = self._setup_workflow_context()")
        
        # Component integration
        method_body_parts.append("    # Component integration")
        method_body_parts.append("    components = self._initialize_workflow_components()")
        
        # Data flow execution
        method_body_parts.append("    # Execute workflow")
        method_body_parts.append(f"    # Workflow: {test_case.scenario.expected_behavior}")
        method_body_parts.append("    result = self._execute_integration_workflow(workflow_context, components)")
        
        # Integration-specific assertions
        method_body_parts.append("    # Integration assertions")
        assertions = self._generate_integration_assertions(test_case)
        method_body_parts.extend(assertions)
        
        method_body = "\n".join(method_body_parts)
        
        return self.templates.method_template.format(
            method_name=method_name,
            method_params="",
            test_description=test_case.description,
            scenario_type=test_case.scenario.scenario_type.value,
            expected_behavior=test_case.scenario.expected_behavior,
            method_body=method_body
        )
    
    def _generate_integration_assertions(self, test_case: TestCase) -> List[str]:
        """Generate assertions for integration test"""
        assertions = []
        
        # Workflow completion assertion
        assertions.append(self.templates.assertion_templates["not_none"])
        assertions.append(self.templates.assertion_templates["success"])
        
        # Data flow assertions
        assertions.append("    # Verify data flow integrity")
        assertions.append("    assert 'data_flow_completed' in result.status")
        
        # Component interaction assertions
        assertions.append("    # Verify component interactions")
        assertions.append("    assert result.component_interactions > 0")
        
        # End-to-end behavior assertions
        if test_case.scenario.expected_side_effects:
            assertions.append("    # Verify expected side effects")
            for side_effect in test_case.scenario.expected_side_effects:
                assertion = self.templates.assertion_templates["contains"].format(
                    value=side_effect
                )
                assertions.append(assertion)
        
        return assertions
    
    def _generate_workflow_setup_method(self, workflow: str, test_cases: List[TestCase]) -> str:
        """Generate workflow-specific setup method"""
        setup_body_parts = [
            "    # Workflow setup",
            f"    self.workflow_name = '{workflow}'",
            "    self.test_data_dir = self._create_test_data_directory()",
            "    self.output_dir = self._create_output_directory()",
            "    self.workflow_config = self._load_workflow_configuration()"
        ]
        
        # Add workflow-specific setup
        if "conversion" in workflow:
            setup_body_parts.extend([
                "    self.source_datasets = self._prepare_source_datasets()",
                "    self.conversion_config = self._load_conversion_configuration()"
            ])
        elif "validation" in workflow:
            setup_body_parts.extend([
                "    self.validation_rules = self._load_validation_rules()",
                "    self.test_datasets = self._prepare_validation_datasets()"
            ])
        
        setup_body = "\n".join(setup_body_parts)
        
        teardown_body_parts = [
            "    # Workflow teardown",
            "    self._cleanup_test_directories()",
            "    self._reset_workflow_state()"
        ]
        
        teardown_body = "\n".join(teardown_body_parts)
        
        if self.framework == TestFramework.PYTEST:
            return self.templates.setup_template.format(
                setup_body=setup_body,
                teardown_body=teardown_body
            )
        else:
            setup_method = self.templates.setup_template.format(setup_body=setup_body)
            teardown_method = self.templates.teardown_template.format(teardown_body=teardown_body)
            return setup_method + "\n\n" + teardown_method


class PerformanceTestGenerator(BaseTestGenerator):
    """Generates performance tests for timing and resource validation"""
    
    def get_test_level(self) -> TestLevel:
        return TestLevel.PERFORMANCE
    
    def generate_tests(self, test_cases: List[TestCase], output_dir: str) -> List[GeneratedTestFile]:
        """Generate performance test files"""
        self.logger.info(f"Generating performance tests for {len(test_cases)} test cases")
        
        # Group test cases by performance category
        grouped_cases = self._group_test_cases_by_performance_category(test_cases)
        
        generated_files = []
        
        for category, cases in grouped_cases.items():
            file_content = self._generate_performance_test_file(category, cases)
            
            file_path = os.path.join(output_dir, f"test_performance_{category.lower()}.py")
            
            generated_file = GeneratedTestFile(
                file_path=file_path,
                framework=self.framework,
                test_count=len(cases),
                test_levels=[TestLevel.PERFORMANCE],
                user_story_id=cases[0].scenario.behavior.name.split('_')[0] if cases else "unknown",
                content=file_content,
                dependencies=self._extract_dependencies(cases) + ["psutil", "memory_profiler"]
            )
            
            generated_files.append(generated_file)
        
        self.logger.info(f"Generated {len(generated_files)} performance test files")
        return generated_files
    
    def _group_test_cases_by_performance_category(self, test_cases: List[TestCase]) -> Dict[str, List[TestCase]]:
        """Group test cases by performance category"""
        groups = {}
        
        for test_case in test_cases:
            if test_case.test_level == TestLevel.PERFORMANCE:
                category = self._determine_performance_category(test_case)
                if category not in groups:
                    groups[category] = []
                groups[category].append(test_case)
        
        return groups
    
    def _determine_performance_category(self, test_case: TestCase) -> str:
        """Determine performance category from test case"""
        behavior_desc = test_case.scenario.behavior.description.lower()
        
        if "memory" in behavior_desc:
            return "memory_usage"
        elif "time" in behavior_desc or "speed" in behavior_desc:
            return "execution_time"
        elif "throughput" in behavior_desc:
            return "throughput"
        elif "scalability" in behavior_desc:
            return "scalability"
        else:
            return "general_performance"
    
    def _generate_performance_test_file(self, category: str, test_cases: List[TestCase]) -> str:
        """Generate performance test file"""
        class_name = self._generate_class_name(f"{category}_performance")
        
        # Generate performance test methods
        test_methods = []
        
        for test_case in test_cases:
            method_code = self._generate_performance_test_method(test_case, category)
            test_methods.append(method_code)
        
        # Generate performance setup method
        perf_setup = self._generate_performance_setup_method(category)
        
        # Combine class body
        class_body = perf_setup + "\n\n" + "\n\n".join(test_methods)
        
        # Generate complete class
        class_code = self.templates.class_template.format(
            class_name=class_name,
            description=f"{category} performance tests",
            user_story_id=test_cases[0].scenario.behavior.name.split('_')[0] if test_cases else "unknown",
            test_level="Performance",
            class_body=self._indent_code(class_body, 1)
        )
        
        # Add imports and file header
        performance_imports = [
            "import psutil",
            "import gc",
            "from memory_profiler import profile",
            "import cProfile",
            "import pstats"
        ]
        
        file_content = self._generate_file_header(category, "performance") + "\n\n"
        file_content += self._generate_imports(performance_imports) + "\n\n"
        file_content += class_code
        
        if self.framework == TestFramework.UNITTEST:
            file_content += "\n\nif __name__ == '__main__':\n    unittest.main()"
        
        return file_content
    
    def _generate_performance_test_method(self, test_case: TestCase, category: str) -> str:
        """Generate performance test method"""
        method_name = self._generate_method_name(test_case)
        
        # Generate method body for performance test
        method_body_parts = []
        
        # Performance test setup
        method_body_parts.append("    # Performance test setup")
        method_body_parts.append("    self._prepare_performance_test_environment()")
        method_body_parts.append("    test_data = self._create_performance_test_data()")
        
        # Resource monitoring setup
        if category == "memory_usage":
            method_body_parts.append("    # Memory monitoring setup")
            method_body_parts.append("    gc.collect()  # Clear garbage before measurement")
            method_body_parts.append("    initial_memory = psutil.Process().memory_info().rss")
        elif category == "execution_time":
            method_body_parts.append("    # Timing setup")
            method_body_parts.append("    start_time = time.perf_counter()")
        
        # Execute performance test
        method_body_parts.append("    # Execute performance test")
        method_body_parts.append(f"    # Performance target: {test_case.scenario.expected_behavior}")
        method_body_parts.append("    result = self._execute_performance_operation(test_data)")
        
        # Resource measurement
        if category == "memory_usage":
            method_body_parts.append("    # Memory measurement")
            method_body_parts.append("    final_memory = psutil.Process().memory_info().rss")
            method_body_parts.append("    memory_usage = final_memory - initial_memory")
        elif category == "execution_time":
            method_body_parts.append("    # Timing measurement")
            method_body_parts.append("    execution_time = time.perf_counter() - start_time")
        
        # Performance assertions
        method_body_parts.append("    # Performance assertions")
        assertions = self._generate_performance_assertions(test_case, category)
        method_body_parts.extend(assertions)
        
        method_body = "\n".join(method_body_parts)
        
        return self.templates.method_template.format(
            method_name=method_name,
            method_params="",
            test_description=test_case.description,
            scenario_type=test_case.scenario.scenario_type.value,
            expected_behavior=test_case.scenario.expected_behavior,
            method_body=method_body
        )
    
    def _generate_performance_assertions(self, test_case: TestCase, category: str) -> List[str]:
        """Generate performance-specific assertions"""
        assertions = []
        
        # Basic success assertion
        assertions.append(self.templates.assertion_templates["not_none"])
        assertions.append(self.templates.assertion_templates["success"])
        
        # Category-specific assertions
        if category == "execution_time":
            for perf_req in test_case.scenario.performance_requirements:
                time_limit = self._extract_time_limit_from_requirement(perf_req)
                assertion = self.templates.assertion_templates["time_limit"].format(
                    limit=time_limit
                )
                assertions.append(assertion)
        elif category == "memory_usage":
            assertions.append("    # Memory usage assertions")
            assertions.append("    max_memory_mb = 1024  # 1GB limit")
            assertions.append("    assert memory_usage / (1024*1024) <= max_memory_mb, f'Memory usage {memory_usage/(1024*1024):.1f}MB exceeds limit {max_memory_mb}MB'")
        
        # Resource efficiency assertions
        assertions.append("    # Resource efficiency verification")
        assertions.append("    assert result.resource_efficiency > 0.5, 'Resource efficiency should be acceptable'")
        
        return assertions
    
    def _generate_performance_setup_method(self, category: str) -> str:
        """Generate performance-specific setup method"""
        setup_body_parts = [
            "    # Performance test setup",
            f"    self.performance_category = '{category}'",
            "    self.performance_thresholds = self._load_performance_thresholds()",
            "    self.monitoring_config = self._configure_performance_monitoring()"
        ]
        
        if category == "memory_usage":
            setup_body_parts.extend([
                "    # Memory monitoring setup",
                "    self.memory_profiler = self._setup_memory_profiler()"
            ])
        elif category == "execution_time":
            setup_body_parts.extend([
                "    # Timing setup",
                "    self.timer_precision = time.get_clock_info('perf_counter').resolution"
            ])
        
        setup_body = "\n".join(setup_body_parts)
        
        teardown_body_parts = [
            "    # Performance test teardown",
            "    self._cleanup_performance_monitoring()",
            "    self._save_performance_metrics()"
        ]
        
        teardown_body = "\n".join(teardown_body_parts)
        
        if self.framework == TestFramework.PYTEST:
            return self.templates.setup_template.format(
                setup_body=setup_body,
                teardown_body=teardown_body
            )
        else:
            setup_method = self.templates.setup_template.format(setup_body=setup_body)
            teardown_method = self.templates.teardown_template.format(teardown_body=teardown_body)
            return setup_method + "\n\n" + teardown_method


class UserAcceptanceTestGenerator(BaseTestGenerator):
    """Generates user acceptance tests for end-to-end workflow validation"""
    
    def get_test_level(self) -> TestLevel:
        return TestLevel.USER_ACCEPTANCE
    
    def generate_tests(self, test_cases: List[TestCase], output_dir: str) -> List[GeneratedTestFile]:
        """Generate user acceptance test files"""
        self.logger.info(f"Generating user acceptance tests for {len(test_cases)} test cases")
        
        # Group test cases by user workflow
        grouped_cases = self._group_test_cases_by_user_workflow(test_cases)
        
        generated_files = []
        
        for workflow, cases in grouped_cases.items():
            file_content = self._generate_user_acceptance_test_file(workflow, cases)
            
            file_path = os.path.join(output_dir, f"test_acceptance_{workflow.lower()}.py")
            
            generated_file = GeneratedTestFile(
                file_path=file_path,
                framework=self.framework,
                test_count=len(cases),
                test_levels=[TestLevel.USER_ACCEPTANCE],
                user_story_id=cases[0].scenario.behavior.name.split('_')[0] if cases else "unknown",
                content=file_content,
                dependencies=self._extract_dependencies(cases) + ["selenium", "requests"]
            )
            
            generated_files.append(generated_file)
        
        self.logger.info(f"Generated {len(generated_files)} user acceptance test files")
        return generated_files
    
    def _group_test_cases_by_user_workflow(self, test_cases: List[TestCase]) -> Dict[str, List[TestCase]]:
        """Group test cases by user workflow"""
        groups = {}
        
        for test_case in test_cases:
            if test_case.test_level == TestLevel.USER_ACCEPTANCE:
                workflow = self._extract_user_workflow(test_case)
                if workflow not in groups:
                    groups[workflow] = []
                groups[workflow].append(test_case)
        
        return groups
    
    def _extract_user_workflow(self, test_case: TestCase) -> str:
        """Extract user workflow from test case"""
        behavior_name = test_case.scenario.behavior.name.lower()
        
        # Map to user workflows from user stories
        if "dataset_curator" in behavior_name or "conversion" in behavior_name:
            return "dataset_curator_workflow"
        elif "biomechanical_validation" in behavior_name or "specialist" in behavior_name:
            return "validation_specialist_workflow"
        elif "administrator" in behavior_name or "release" in behavior_name:
            return "system_admin_workflow"
        elif "collaborative" in behavior_name or "multi_role" in behavior_name:
            return "collaborative_workflow"
        else:
            return "general_user_workflow"
    
    def _generate_user_acceptance_test_file(self, workflow: str, test_cases: List[TestCase]) -> str:
        """Generate user acceptance test file"""
        class_name = self._generate_class_name(f"{workflow}_acceptance")
        
        # Generate user acceptance test methods
        test_methods = []
        
        for test_case in test_cases:
            method_code = self._generate_user_acceptance_test_method(test_case, workflow)
            test_methods.append(method_code)
        
        # Generate user workflow setup method
        workflow_setup = self._generate_user_workflow_setup_method(workflow)
        
        # Combine class body
        class_body = workflow_setup + "\n\n" + "\n\n".join(test_methods)
        
        # Generate complete class
        class_code = self.templates.class_template.format(
            class_name=class_name,
            description=f"{workflow} user acceptance tests",
            user_story_id=test_cases[0].scenario.behavior.name.split('_')[0] if test_cases else "unknown",
            test_level="User Acceptance",
            class_body=self._indent_code(class_body, 1)
        )
        
        # Add imports and file header
        acceptance_imports = [
            "import subprocess",
            "import requests",
            "from pathlib import Path"
        ]
        
        file_content = self._generate_file_header(workflow, "user_acceptance") + "\n\n"
        file_content += self._generate_imports(acceptance_imports) + "\n\n"
        file_content += class_code
        
        if self.framework == TestFramework.UNITTEST:
            file_content += "\n\nif __name__ == '__main__':\n    unittest.main()"
        
        return file_content
    
    def _generate_user_acceptance_test_method(self, test_case: TestCase, workflow: str) -> str:
        """Generate user acceptance test method"""
        method_name = self._generate_method_name(test_case)
        
        # Generate method body for user acceptance test
        method_body_parts = []
        
        # User workflow setup
        method_body_parts.append("    # User workflow setup")
        method_body_parts.append("    user_context = self._setup_user_context()")
        method_body_parts.append("    workflow_environment = self._prepare_workflow_environment()")
        
        # End-to-end workflow execution
        method_body_parts.append("    # Execute end-to-end user workflow")
        method_body_parts.append(f"    # User goal: {test_case.scenario.expected_behavior}")
        method_body_parts.append("    workflow_result = self._execute_user_workflow(user_context, workflow_environment)")
        
        # User satisfaction verification
        method_body_parts.append("    # Verify user satisfaction criteria")
        assertions = self._generate_user_acceptance_assertions(test_case, workflow)
        method_body_parts.extend(assertions)
        
        method_body = "\n".join(method_body_parts)
        
        return self.templates.method_template.format(
            method_name=method_name,
            method_params="",
            test_description=test_case.description,
            scenario_type=test_case.scenario.scenario_type.value,
            expected_behavior=test_case.scenario.expected_behavior,
            method_body=method_body
        )
    
    def _generate_user_acceptance_assertions(self, test_case: TestCase, workflow: str) -> List[str]:
        """Generate user acceptance assertions"""
        assertions = []
        
        # Workflow completion assertion
        assertions.append(self.templates.assertion_templates["not_none"])
        assertions.append(self.templates.assertion_templates["success"])
        
        # User satisfaction assertions
        assertions.append("    # User satisfaction verification")
        assertions.append("    assert workflow_result.user_satisfaction >= 4.0, 'User satisfaction should be >= 4/5'")
        
        # Workflow quality assertions
        assertions.append("    # Workflow quality verification")
        assertions.append("    assert workflow_result.quality_score >= 0.9, 'Workflow quality should be >= 90%'")
        
        # Expected outcomes verification
        if test_case.scenario.expected_side_effects:
            assertions.append("    # Expected outcomes verification")
            for side_effect in test_case.scenario.expected_side_effects:
                assertion = self.templates.assertion_templates["contains"].format(
                    value=side_effect
                )
                assertions.append(assertion)
        
        # Workflow-specific assertions
        if "curator" in workflow:
            assertions.append("    # Dataset curator specific assertions")
            assertions.append("    assert workflow_result.dataset_converted, 'Dataset should be successfully converted'")
            assertions.append("    assert workflow_result.quality_report_generated, 'Quality report should be generated'")
        elif "specialist" in workflow:
            assertions.append("    # Validation specialist specific assertions")
            assertions.append("    assert workflow_result.validation_completed, 'Validation should be completed'")
            assertions.append("    assert workflow_result.ranges_updated, 'Validation ranges should be updated'")
        elif "admin" in workflow:
            assertions.append("    # System administrator specific assertions")
            assertions.append("    assert workflow_result.release_prepared, 'Release should be prepared'")
            assertions.append("    assert workflow_result.benchmarks_created, 'Benchmarks should be created'")
        
        return assertions
    
    def _generate_user_workflow_setup_method(self, workflow: str) -> str:
        """Generate user workflow setup method"""
        setup_body_parts = [
            "    # User workflow setup",
            f"    self.workflow_type = '{workflow}'",
            "    self.user_environment = self._create_user_environment()",
            "    self.test_scenarios = self._load_user_test_scenarios()"
        ]
        
        # Workflow-specific setup
        if "curator" in workflow:
            setup_body_parts.extend([
                "    # Dataset curator setup",
                "    self.source_datasets = self._prepare_curator_test_datasets()",
                "    self.conversion_tools = self._setup_conversion_tools()"
            ])
        elif "specialist" in workflow:
            setup_body_parts.extend([
                "    # Validation specialist setup",
                "    self.validation_datasets = self._prepare_validation_test_datasets()",
                "    self.validation_tools = self._setup_validation_tools()"
            ])
        elif "admin" in workflow:
            setup_body_parts.extend([
                "    # System administrator setup",
                "    self.admin_environment = self._setup_admin_environment()",
                "    self.release_tools = self._setup_release_tools()"
            ])
        
        setup_body = "\n".join(setup_body_parts)
        
        teardown_body_parts = [
            "    # User workflow teardown",
            "    self._cleanup_user_environment()",
            "    self._save_workflow_results()"
        ]
        
        teardown_body = "\n".join(teardown_body_parts)
        
        if self.framework == TestFramework.PYTEST:
            return self.templates.setup_template.format(
                setup_body=setup_body,
                teardown_body=teardown_body
            )
        else:
            setup_method = self.templates.setup_template.format(setup_body=setup_body)
            teardown_method = self.templates.teardown_template.format(teardown_body=teardown_body)
            return setup_method + "\n\n" + teardown_method
    
    # Helper methods shared by all generators
    def _extract_dependencies(self, test_cases: List[TestCase]) -> List[str]:
        """Extract dependencies from test cases"""
        dependencies = set()
        
        for test_case in test_cases:
            # Extract from mock dependencies
            dependencies.update(test_case.mock_dependencies)
            
            # Extract from test data requirements
            for req in test_case.scenario.test_data_requirements:
                if "pandas" in req.lower():
                    dependencies.add("pandas")
                if "numpy" in req.lower():
                    dependencies.add("numpy")
                if "parquet" in req.lower():
                    dependencies.add("pyarrow")
        
        return list(dependencies)
    
    def _extract_time_limit(self, requirement: str, unit: str) -> float:
        """Extract time limit from performance requirement"""
        # Simple regex to extract numbers
        import re
        numbers = re.findall(r'\d+', requirement)
        return float(numbers[0]) if numbers else (120 if unit == "seconds" else 2)
    
    def _extract_time_limit_from_requirement(self, requirement: str) -> float:
        """Extract time limit from performance requirement string"""
        requirement_lower = requirement.lower()
        
        if "minutes" in requirement_lower:
            return self._extract_time_limit(requirement, "minutes") * 60
        elif "seconds" in requirement_lower:
            return self._extract_time_limit(requirement, "seconds")
        else:
            return 120  # Default 2 minutes
    
    def _indent_code(self, code: str, levels: int) -> str:
        """Indent code by specified levels"""
        if not code.strip():
            return code
        
        indent = "    " * levels
        lines = code.split('\n')
        indented_lines = [indent + line if line.strip() else line for line in lines]
        return '\n'.join(indented_lines)
    
    def _generate_file_header(self, component: str, test_type: str) -> str:
        """Generate file header with documentation"""
        return f'''#!/usr/bin/env python3
"""
{test_type.replace('_', ' ').title()} Tests for {component.replace('_', ' ').title()}

Generated by Test Agent Framework
Created: 2025-06-16

Purpose: {test_type.replace('_', ' ').title()} tests ensuring behavioral compliance without implementation dependencies

Intent: These tests validate external behavior and interface contracts based solely on
requirements and specifications, ensuring system functionality meets user expectations.
"""'''


if __name__ == "__main__":
    # Example usage
    from test_agent_framework import TestCase, TestScenario, TestableBehavior, ScenarioType
    
    # Create example test case
    behavior = TestableBehavior(
        name="us_001_primary_workflow",
        description="Convert raw biomechanical datasets to standardized parquet format efficiently",
        input_conditions=["Source dataset", "Conversion configuration"],
        expected_outputs=["Standardized parquet file"],
        error_conditions=["Invalid source format"],
        performance_requirements=["60 minutes for 500-1000 trials"]
    )
    
    scenario = TestScenario(
        name="dataset_conversion_happy_path",
        scenario_type=ScenarioType.HAPPY_PATH,
        behavior=behavior,
        test_data_requirements=["valid_test_dataset", "conversion_configuration"],
        mock_requirements=["file_system_mock"],
        expected_behavior="Successfully convert dataset to standard format",
        expected_side_effects=["Conversion completed", "Quality validation passed"]
    )
    
    test_case = TestCase(
        name="test_dataset_conversion_workflow",
        description="Test dataset conversion workflow functionality",
        test_level=TestLevel.UNIT,
        scenario=scenario,
        setup_code="# Setup test environment",
        execution_code="# Execute conversion",
        assertions=["assert result.success", "assert result.output_format == 'parquet'"]
    )
    
    # Generate tests
    unit_generator = UnitTestGenerator()
    generated_files = unit_generator.generate_tests([test_case], "/tmp/test_output")
    
    print(f"Generated {len(generated_files)} test files")
    for file in generated_files:
        print(f"- {file.file_path}: {file.test_count} tests")