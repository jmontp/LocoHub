# R Package Testing Workflow
# 
# Created: 2025-06-19 with user permission
# Purpose: Comprehensive R package testing with CI/CD integration
#
# Intent: Automated testing of the LocomotionData R package across multiple
# R versions and platforms, with test coverage reporting and performance
# benchmarking for robust package development and maintenance.

name: R Package Test Suite

on:
  push:
    branches: [ main, phase1-implementation, phase0-validation ]
    paths:
      - 'source/lib/r/**'
      - '.github/workflows/r-package-test.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'source/lib/r/**'
      - '.github/workflows/r-package-test.yml'
  schedule:
    # Run daily at 6 AM UTC to catch dependency issues
    - cron: '0 6 * * *'

env:
  R_KEEP_PKG_SOURCE: yes
  R_REMOTES_NO_ERRORS_FROM_WARNINGS: true
  # Use repository RSPM snapshot for faster, more reliable installs
  RSPM: "https://packagemanager.rstudio.com/cran/__linux__/jammy/latest"

jobs:
  # ============================================================================
  # R CMD CHECK - Standard Package Validation
  # ============================================================================
  r-cmd-check:
    runs-on: ${{ matrix.config.os }}
    name: ${{ matrix.config.os }} (R-${{ matrix.config.r }})
    
    strategy:
      fail-fast: false
      matrix:
        config:
          # Test on multiple R versions and platforms
          - {os: ubuntu-latest, r: 'devel', http-user-agent: 'release'}
          - {os: ubuntu-latest, r: 'release'}
          - {os: ubuntu-latest, r: 'oldrel-1'}
          - {os: ubuntu-latest, r: 'oldrel-2'}
          - {os: windows-latest, r: 'release'}
          - {os: macOS-latest, r: 'release'}

    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
      R_KEEP_PKG_SOURCE: yes

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: ${{ matrix.config.r }}
          http-user-agent: ${{ matrix.config.http-user-agent }}
          use-public-rspm: true

      - name: Setup Pandoc
        uses: r-lib/actions/setup-pandoc@v2

      - name: Query R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          working-directory: source/lib/r
          extra-packages: |
            any::rcmdcheck
            any::covr
            any::testthat
            any::devtools
            any::arrow
            any::remotes
          needs: check

      - name: Setup system dependencies (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libcurl4-openssl-dev \
            libssl-dev \
            libxml2-dev \
            libfontconfig1-dev \
            libfreetype6-dev \
            libfribidi-dev \
            libharfbuzz-dev \
            libjpeg-dev \
            libpng-dev \
            libtiff-dev

      - name: Check R package structure
        run: |
          if (!file.exists("source/lib/r/DESCRIPTION")) {
            stop("DESCRIPTION file not found in source/lib/r/")
          }
          if (!dir.exists("source/lib/r/R")) {
            stop("R/ directory not found in source/lib/r/")
          }
          if (!dir.exists("source/lib/r/tests")) {
            stop("tests/ directory not found in source/lib/r/")
          }
          cat("‚úì R package structure validated\n")
        shell: Rscript {0}
        working-directory: source/lib/r

      - name: Run R CMD check
        uses: r-lib/actions/check-r-package@v2
        with:
          working-directory: source/lib/r
          args: 'c("--no-manual", "--as-cran", "--no-multiarch")'
          build_args: 'c("--no-manual", "--compact-vignettes=gs+qpdf")'
          error-on: '"warning"'
          check-dir: '"check"'

      - name: Show testthat output
        if: always()
        run: |
          find . -name 'testthat.Rout*' -exec cat '{}' \; || true
        shell: bash
        working-directory: source/lib/r

      - name: Upload check results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ runner.os }}-r${{ matrix.config.r }}-results
          path: source/lib/r/check

  # ============================================================================
  # Test Coverage Analysis
  # ============================================================================
  test-coverage:
    runs-on: ubuntu-latest
    name: Test Coverage Analysis
    
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Setup R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          working-directory: source/lib/r
          extra-packages: |
            any::covr
            any::testthat
            any::devtools
            any::DT

      - name: Generate test coverage report
        run: |
          # Install package in development mode
          devtools::load_all()
          
          # Run coverage analysis
          coverage <- covr::package_coverage(
            type = c("tests", "examples"),
            quiet = FALSE,
            clean = FALSE
          )
          
          # Print coverage summary
          print(coverage)
          cat("\nCoverage Summary:\n")
          print(covr::percent_coverage(coverage))
          
          # Generate detailed coverage report
          covr::report(
            coverage, 
            file = "coverage-report.html",
            browse = FALSE
          )
          
          # Check coverage threshold
          coverage_percent <- covr::percent_coverage(coverage)
          cat(sprintf("\nTotal coverage: %.2f%%\n", coverage_percent))
          
          # Set coverage threshold (adjust as needed)
          threshold <- 75
          if (coverage_percent < threshold) {
            cat(sprintf("‚ùå Coverage %.2f%% is below threshold of %d%%\n", 
                       coverage_percent, threshold))
            # For now, just warn - don't fail the build
            # quit(status = 1)
          } else {
            cat(sprintf("‚úÖ Coverage %.2f%% meets threshold of %d%%\n", 
                       coverage_percent, threshold))
          }
        shell: Rscript {0}
        working-directory: source/lib/r

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            source/lib/r/coverage-report.html
          retention-days: 30

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        run: |
          # Generate coverage summary for PR comment
          coverage <- covr::package_coverage(quiet = TRUE)
          coverage_percent <- covr::percent_coverage(coverage)
          
          cat("üìä **R Package Test Coverage Summary**\n\n")
          cat(sprintf("- **Overall Coverage:** %.2f%%\n", coverage_percent))
          cat("- **Threshold:** 75%\n")
          
          if (coverage_percent >= 75) {
            cat("- **Status:** ‚úÖ Meets threshold\n")
          } else {
            cat("- **Status:** ‚ö†Ô∏è Below threshold\n")
          }
          
          cat("\nDetailed coverage report available in build artifacts.\n")
        shell: Rscript {0}
        working-directory: source/lib/r

  # ============================================================================
  # Performance Benchmarking
  # ============================================================================
  performance-benchmarks:
    runs-on: ubuntu-latest
    name: Performance Benchmarks
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: Setup R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          working-directory: source/lib/r
          extra-packages: |
            any::microbenchmark
            any::pryr
            any::ggplot2
            any::data.table

      - name: Run performance benchmarks
        run: |
          # Load the package
          devtools::load_all()
          
          cat("üöÄ Running Performance Benchmarks\n")
          cat("=" , rep("=", 35), "\n", sep = "")
          
          # Create performance test data
          set.seed(42)
          
          # Small dataset performance
          small_data <- generateSyntheticGaitData(
            n_subjects = 2, n_tasks = 2, n_cycles = 5
          )
          temp_small <- tempfile(fileext = ".parquet")
          arrow::write_parquet(small_data, temp_small)
          
          # Medium dataset performance  
          medium_data <- generateSyntheticGaitData(
            n_subjects = 10, n_tasks = 3, n_cycles = 10
          )
          temp_medium <- tempfile(fileext = ".parquet")
          arrow::write_parquet(medium_data, temp_medium)
          
          # Benchmark constructor performance
          cat("\nüì¶ Constructor Performance:\n")
          constructor_bench <- microbenchmark::microbenchmark(
            small = new("LocomotionData", data_path = temp_small),
            medium = new("LocomotionData", data_path = temp_medium),
            times = 5
          )
          print(constructor_bench)
          
          # Create objects for method benchmarks
          small_obj <- new("LocomotionData", data_path = temp_small)
          medium_obj <- new("LocomotionData", data_path = temp_medium)
          
          subject_small <- getSubjects(small_obj)[1]
          task_small <- getTasks(small_obj)[1]
          subject_medium <- getSubjects(medium_obj)[1] 
          task_medium <- getTasks(medium_obj)[1]
          
          # Benchmark core methods
          cat("\n‚ö° Core Methods Performance:\n")
          methods_bench <- microbenchmark::microbenchmark(
            getCycles_small = getCycles(small_obj, subject_small, task_small),
            getCycles_medium = getCycles(medium_obj, subject_medium, task_medium),
            getMeanPatterns_small = getMeanPatterns(small_obj, subject_small, task_small),
            getMeanPatterns_medium = getMeanPatterns(medium_obj, subject_medium, task_medium),
            getSummaryStatistics_small = getSummaryStatistics(small_obj, subject_small, task_small),
            getSummaryStatistics_medium = getSummaryStatistics(medium_obj, subject_medium, task_medium),
            times = 3
          )
          print(methods_bench)
          
          # Memory usage analysis
          cat("\nüíæ Memory Usage Analysis:\n")
          initial_memory <- pryr::mem_used()
          cat(sprintf("Initial memory: %s\n", format(initial_memory, units = "auto")))
          
          # Test memory usage with medium dataset
          pre_load_memory <- pryr::mem_used()
          medium_obj_test <- new("LocomotionData", data_path = temp_medium)
          post_load_memory <- pryr::mem_used()
          
          load_memory_diff <- post_load_memory - pre_load_memory
          cat(sprintf("Memory for medium dataset load: %s\n", 
                     format(load_memory_diff, units = "auto")))
          
          # Test method memory usage
          pre_method_memory <- pryr::mem_used()
          cycles_result <- getCycles(medium_obj_test, subject_medium, task_medium)
          post_method_memory <- pryr::mem_used()
          
          method_memory_diff <- post_method_memory - pre_method_memory
          cat(sprintf("Memory for getCycles method: %s\n", 
                     format(method_memory_diff, units = "auto")))
          
          # Performance summary
          cat("\nüìä Performance Summary:\n")
          small_constructor_time <- median(constructor_bench$time[constructor_bench$expr == "small"]) / 1e6
          medium_constructor_time <- median(constructor_bench$time[constructor_bench$expr == "medium"]) / 1e6
          
          cat(sprintf("- Small dataset constructor: %.2f ms\n", small_constructor_time))
          cat(sprintf("- Medium dataset constructor: %.2f ms\n", medium_constructor_time))
          cat(sprintf("- Memory efficiency: %s per dataset\n", 
                     format(load_memory_diff, units = "auto")))
          
          # Set performance thresholds (can be adjusted)
          if (medium_constructor_time > 5000) {  # 5 seconds
            cat("‚ö†Ô∏è  Constructor performance may need optimization\n")
          } else {
            cat("‚úÖ Constructor performance is acceptable\n")
          }
          
          # Cleanup
          unlink(c(temp_small, temp_medium))
          
          cat("\nüéØ Benchmark completed successfully!\n")
        shell: Rscript {0}
        working-directory: source/lib/r

  # ============================================================================
  # Documentation and Examples Testing
  # ============================================================================
  test-documentation:
    runs-on: ubuntu-latest
    name: Documentation and Examples
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2

      - name: Setup R dependencies
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          working-directory: source/lib/r
          extra-packages: |
            any::roxygen2
            any::pkgdown
            any::rmarkdown

      - name: Test documentation generation
        run: |
          # Generate documentation
          roxygen2::roxygenise()
          cat("‚úÖ Roxygen documentation generated successfully\n")
          
          # Check for documentation completeness
          man_files <- list.files("man", pattern = "\\.Rd$", full.names = TRUE)
          cat(sprintf("üìö Generated %d documentation files\n", length(man_files)))
          
          # Validate that key functions have documentation
          key_functions <- c(
            "LocomotionData-class", "getCycles", "getMeanPatterns", 
            "validateCycles", "getSummaryStatistics"
          )
          
          documented_functions <- tools::file_path_sans_ext(basename(man_files))
          missing_docs <- setdiff(key_functions, documented_functions)
          
          if (length(missing_docs) > 0) {
            cat("‚ö†Ô∏è  Missing documentation for:", paste(missing_docs, collapse = ", "), "\n")
          } else {
            cat("‚úÖ All key functions have documentation\n")
          }
        shell: Rscript {0}
        working-directory: source/lib/r

      - name: Test examples in documentation
        run: |
          # Test examples in documentation files
          example_files <- list.files("man", pattern = "\\.Rd$", full.names = TRUE)
          
          cat("üß™ Testing documentation examples\n")
          
          # Simple example testing (could be expanded)
          for (rd_file in head(example_files, 5)) {  # Test first 5 files
            tryCatch({
              tools::Rd2ex(rd_file, tempfile(fileext = ".R"))
              cat(sprintf("‚úÖ Examples in %s are valid\n", basename(rd_file)))
            }, error = function(e) {
              cat(sprintf("‚ö†Ô∏è  Issue with examples in %s: %s\n", basename(rd_file), e$message))
            })
          }
        shell: Rscript {0}
        working-directory: source/lib/r

  # ============================================================================
  # Integration with Main Test Suite
  # ============================================================================
  integration-summary:
    runs-on: ubuntu-latest
    name: Integration Summary
    needs: [r-cmd-check, test-coverage, performance-benchmarks, test-documentation]
    if: always()
    
    steps:
      - name: Generate summary report
        run: |
          echo "# üìã R Package Test Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results Overview" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # R CMD CHECK status
          if [[ "${{ needs.r-cmd-check.result }}" == "success" ]]; then
            echo "‚úÖ **R CMD CHECK**: All platforms passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **R CMD CHECK**: Some platforms failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Coverage status  
          if [[ "${{ needs.test-coverage.result }}" == "success" ]]; then
            echo "‚úÖ **Test Coverage**: Analysis completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Test Coverage**: Analysis failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Performance status
          if [[ "${{ needs.performance-benchmarks.result }}" == "success" ]]; then
            echo "‚úÖ **Performance**: Benchmarks completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Performance**: Benchmarks failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Documentation status
          if [[ "${{ needs.test-documentation.result }}" == "success" ]]; then
            echo "‚úÖ **Documentation**: Examples and docs validated" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Documentation**: Validation failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üìä Build Matrix Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Ubuntu (Latest R)**: Primary development platform" >> $GITHUB_STEP_SUMMARY
          echo "- **Windows (Latest R)**: Cross-platform compatibility" >> $GITHUB_STEP_SUMMARY
          echo "- **macOS (Latest R)**: Cross-platform compatibility" >> $GITHUB_STEP_SUMMARY
          echo "- **R Development Version**: Future compatibility" >> $GITHUB_STEP_SUMMARY
          echo "- **R Old Release**: Backward compatibility" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üîó Related Workflows" >> $GITHUB_STEP_SUMMARY
          echo "This R package testing complements the main Python test suite and documentation workflows." >> $GITHUB_STEP_SUMMARY
        shell: bash

      - name: Set overall status
        run: |
          if [[ "${{ needs.r-cmd-check.result }}" == "success" && 
                "${{ needs.test-coverage.result }}" == "success" && 
                "${{ needs.performance-benchmarks.result }}" == "success" && 
                "${{ needs.test-documentation.result }}" == "success" ]]; then
            echo "üéâ All R package tests passed!"
            exit 0
          else
            echo "‚ö†Ô∏è Some R package tests failed - check individual job results"
            exit 1
          fi