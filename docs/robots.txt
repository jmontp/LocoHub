# Robots.txt for Locomotion Data Standardization Documentation
# Created: 2025-06-19 with user permission
# Purpose: SEO optimization and crawling guidance for research documentation

# Allow all bots to access the documentation
User-agent: *
Allow: /

# Optimize crawling for major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 2

# Academic and research crawlers
User-agent: ia_archiver
Allow: /

User-agent: Wayback
Allow: /

User-agent: archive.org_bot
Allow: /

# Disallow crawling of administrative and build files
Disallow: /admin/
Disallow: /.git/
Disallow: /build/
Disallow: /node_modules/
Disallow: /venv/
Disallow: /conda_env/
Disallow: /__pycache__/
Disallow: /.pytest_cache/
Disallow: /htmlcov/
Disallow: /.coverage
Disallow: *.log
Disallow: *.tmp
Disallow: *.temp

# Disallow crawling of development and staging environments
Disallow: /dev/
Disallow: /staging/
Disallow: /test/
Disallow: /debug/

# Disallow crawling of private or work-in-progress content
Disallow: /private/
Disallow: /wip/
Disallow: /draft/

# Allow crawling of important API endpoints for better indexing
Allow: /api/
Allow: /reference/
Allow: /tutorials/
Allow: /getting_started/
Allow: /examples/
Allow: /datasets/

# Sitemap location
Sitemap: https://locomotion-data-standardization.readthedocs.io/sitemap.xml
Sitemap: https://locomotion-data-standardization.readthedocs.io/sitemap-images.xml

# Additional crawling guidelines for research content
# Encourage deep crawling of educational content
User-agent: *
Allow: /tutorials/*
Allow: /examples/*
Allow: /reference/*
Allow: /getting_started/*

# Research-specific bot configurations
User-agent: CiteSeerXBot
Allow: /
Crawl-delay: 5

User-agent: ResearchGate
Allow: /
Crawl-delay: 3

User-agent: ScholarBot
Allow: /
Crawl-delay: 2

# Social media crawlers for better sharing
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

# Performance optimization hints
# Suggest crawling during off-peak hours (UTC)
# Request-rate: 1/10s  # One request every 10 seconds
# Visit-time: 0200-0600  # Prefer crawling between 2-6 AM UTC

# Cache-Control suggestions for crawlers
# Suggested crawl frequency:
# Homepage and main sections: daily
# Documentation pages: weekly  
# API reference: weekly
# Tutorial pages: bi-weekly
# Dataset pages: monthly

# Contact information for crawling issues
# Contact: webmaster@locomotion-data-standardization.org
# Technical support: support@locomotion-data-standardization.org