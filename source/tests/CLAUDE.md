# CLAUDE.md - Tests Directory

This file provides Claude Code guidance for working with the testing framework in this directory.

## Directory Overview

The `source/tests/` directory contains the comprehensive testing and validation framework for the Locomotion Data Standardization project, including unit tests, integration tests, demonstration scripts, and validation systems.

## Directory Structure

```
source/tests/
â”œâ”€â”€ test_{module}.py           # Unit and integration test suites
â”œâ”€â”€ demo_{module}.py           # Demonstration scripts with live examples
â”œâ”€â”€ sample_plots/              # Generated plots organized by module
â”‚   â”œâ”€â”€ demo_{module}/         # Plots generated by demo scripts
â”‚   â””â”€â”€ {other_plots}/         # Standalone example plots
â”œâ”€â”€ validation_blueprint*.py   # Core validation systems
â”œâ”€â”€ enhanced_validation*.py    # Advanced validation tools
â””â”€â”€ spec_compliance*.py        # Specification compliance testing
```

## File Creation Requirements
**CRITICAL**: All file creation in the tests directory requires explicit user permission.

**Before creating ANY new file:**
1. **Ask for permission**: "May I create a new test file: `test_{module_name}.py`?"
2. **Explain intent**: Clearly state what the file will test and why it's needed
3. **Wait for approval**: Do not proceed without explicit user consent

**Required documentation for new test files:**
- Creation date with permission statement
- Comprehensive intent description
- Clear explanation of what functionality is being tested
- Usage examples and integration patterns

## Testing Conventions

**IMPORTANT FILE DISTINCTIONS:**
- **`test_*.py` files**: Headless validation of libraries for automated testing (no visual outputs)
- **`demo_*.py` files**: Generate plots and visual outputs for user observation and documentation

### 1. Test File Naming (`test_{module}.py`)
**Purpose**: Headless validation of library functionality using pytest framework for automated testing
**Permission Required**: YES - Must ask user before creating

**Intent**: Test files are designed for automated, headless validation of libraries without generating visual outputs. They focus on unit testing, integration testing, and performance validation that can run in CI/CD pipelines.

**Naming Pattern**: `test_{module_name}.py`
- `test_step_classifier.py` - Tests for `validation/step_classifier.py`
- `test_filters_by_phase_plots.py` - Tests for `validation/filters_by_phase_plots.py`
- `test_dataset_validator.py` - Tests for `validation/dataset_validator.py`

**Structure Requirements**:
```python
#!/usr/bin/env python3
"""
Comprehensive Test Suite for {module_name}.py

Created: YYYY-MM-DD
Purpose: Test all functionality of the {ModuleName} module including {key_features}.

Intent:
This test suite validates {main_purpose}:

1. **{Feature1}**: Tests {description}
2. **{Feature2}**: Tests {description}
3. **Edge Cases**: Tests {description}
4. **Integration**: Tests {description}
5. **Performance**: Tests {description}

Test Categories:
- Unit tests for individual methods
- Integration tests with realistic data
- Edge case tests for robustness
- Performance tests for scalability
"""

# Pytest compatibility with fallback
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False
    class pytest:
        @staticmethod
        def fixture(func):
            return func
```

**Test Method Patterns**:
- `test_initialization()` - Test module initialization
- `test_{method_name}_basic()` - Basic functionality tests
- `test_{method_name}_edge_cases()` - Edge case handling
- `test_{method_name}_integration()` - Integration scenarios
- `test_error_handling()` - Error and exception handling
- `test_performance()` - Performance with large datasets

### 2. Demo File Naming (`demo_{module}.py`)
**Purpose**: Generate plots for user observation and interactive demonstrations showing real-world usage
**Permission Required**: YES - Must ask user before creating

**Intent**: Demo files are designed to generate visual outputs, plots, and interactive demonstrations that users can observe to understand functionality. They create comprehensive visual documentation and examples of library capabilities.

**Naming Pattern**: `demo_{module_name}.py`
- `demo_step_classifier.py` - Demonstration of step classification functionality
- `demo_filters_by_phase_plots.py` - Demonstration of filters by phase plotting

**Structure Requirements**:
```python
#!/usr/bin/env python3
"""
Demonstration Script for {module_name}.py

Created: YYYY-MM-DD
Purpose: Comprehensive demonstration of the {description} showing
         how to use the {ModuleName} module for {primary_use_case}.

Intent:
This script demonstrates the key features of {system_name}:

1. **{Feature1}**: {Description}
2. **{Feature2}**: {Description}
3. **{Feature3}**: {Description}
4. **Integration Examples**: {Description}

Output:
Generates demonstration plots in source/tests/sample_plots/demo_{module_name}/:
- {Plot1 description}
- {Plot2 description}
- {Plot3 description}

Usage:
    python3 source/tests/demo_{module_name}.py

This demo helps developers understand:
- {Understanding1}
- {Understanding2}
- {Integration patterns}
"""

def demo_{feature1}():
    """Demonstrate {feature1} functionality."""
    print_banner("{Feature1} Demonstration")
    # Implementation with detailed explanations

def demo_{feature2}():
    """Demonstrate {feature2} functionality."""
    print_banner("{Feature2} Demonstration")
    # Implementation with detailed explanations

def main():
    """Run all demonstrations."""
    print("ðŸŽ¨ {Module} Demonstration")
    print("="*60)
    
    demo_{feature1}()
    demo_{feature2}()
    # ... more demos
    
    print_banner("Demo Complete")
    print("ðŸŽ‰ Demo Complete! Generated X plots...")

if __name__ == "__main__":
    main()
```

### 3. Sample Plots Directory Structure
**Purpose**: Organized storage of generated plots and visual outputs

**Directory Pattern**: `sample_plots/demo_{module_name}/`
- `sample_plots/demo_step_classifier/` - Plots from step classifier demo
- `sample_plots/demo_filters_by_phase_plots/` - Plots from filters demo
- `sample_plots/demo_dataset_validator/` - Plots from validator demo

**Plot Organization**:
```python
# In demo scripts - use consistent output directory pattern
output_dir = Path(__file__).parent / "sample_plots" / "demo_{module_name}"
output_dir.mkdir(parents=True, exist_ok=True)
```

**Plot Naming Conventions**:
- Use descriptive prefixes: `1_baseline_`, `2_with_data_`, `3_violations_`
- Include scenario description: `summary_classification_`, `feature_specific_`
- Follow module patterns: `{scenario}_{task}_{mode}_filters_by_phase.png`

## Test Categories and Standards

### Unit Tests
**Purpose**: Test individual methods and functions in isolation

**Requirements**:
- Test all public methods and functions
- Include parameter validation testing
- Test return value types and formats
- Cover all code paths and conditions
- Use descriptive assertion messages

**Example**:
```python
def test_classify_steps_for_feature_basic(self, classifier, sample_failures, sample_mapping):
    """Test basic step classification for a specific feature."""
    colors = classifier.classify_steps_for_feature(
        sample_failures, sample_mapping, 'hip_flexion_angle_ipsi', 'kinematic'
    )
    
    assert isinstance(colors, np.ndarray), "Should return numpy array"
    assert len(colors) == len(sample_mapping), "Should have one color per step"
    assert all(c in ['gray', 'red', 'pink'] for c in colors), "Should use valid colors"
```

### Integration Tests
**Purpose**: Test module interactions and realistic workflows

**Requirements**:
- Use realistic data scenarios
- Test cross-module interactions
- Validate end-to-end workflows
- Include performance considerations
- Test with multiple data formats

**Example**:
```python
def test_integration_with_plotting_system(self, classifier, validation_data):
    """Test step classifier integration with plotting system."""
    from validation.filters_by_phase_plots import create_filters_by_phase_plot
    
    step_colors = classifier.classify_steps_for_feature(...)
    plot_path = create_filters_by_phase_plot(
        validation_data=validation_data,
        data=test_data,
        step_colors=step_colors
    )
    
    assert os.path.exists(plot_path), "Plot should be generated"
```

### Edge Case Tests
**Purpose**: Test boundary conditions and error handling

**Requirements**:
- Test with empty inputs
- Test with invalid parameters
- Test with extreme values
- Test memory limits
- Test error propagation

**Example**:
```python
def test_empty_input_handling(self, classifier):
    """Test handling of empty inputs."""
    colors = classifier.classify_steps_for_feature([], {}, 'hip_flexion_angle_ipsi', 'kinematic')
    assert len(colors) == 0, "Should handle empty inputs gracefully"
```

### Performance Tests
**Purpose**: Validate performance with large datasets

**Requirements**:
- Test with realistic dataset sizes (1000+ steps)
- Measure execution time
- Monitor memory usage
- Identify performance bottlenecks
- Set performance benchmarks

**Example**:
```python
def test_large_dataset_performance(self, classifier):
    """Test performance with large number of steps."""
    large_mapping = {i: 'level_walking' for i in range(1000)}
    
    import time
    start_time = time.time()
    colors = classifier.classify_steps_for_feature(failures, large_mapping, feature, mode)
    execution_time = time.time() - start_time
    
    assert execution_time < 1.0, f"Should complete in <1s, took {execution_time:.3f}s"
    assert len(colors) == 1000, "Should handle large datasets"
```

## Demo Script Standards

### Interactive Demonstrations
**Purpose**: Show real-world usage with visual feedback

**Requirements**:
- Include detailed explanations of each step
- Generate actual visual outputs (plots, reports)
- Use realistic data scenarios
- Show both success and failure cases
- Demonstrate integration patterns

### Educational Value
**Purpose**: Help developers understand module usage

**Requirements**:
- Explain the "why" behind each feature
- Show progression from simple to complex scenarios
- Include performance demonstrations
- Show best practices and common patterns
- Provide troubleshooting examples

### Visual Output Standards
**Purpose**: Generate publication-ready demonstrations

**Requirements**:
- Use descriptive plot titles and labels
- Include legends and explanations
- Generate multiple scenario examples
- Show data quality indicators
- Create before/after comparisons

## Validation Testing Framework

### Core Validation Tests (`validation_blueprint*.py`)
**Purpose**: Test the 5-layer validation system

**Layers Tested**:
1. **Pre-checks** (Codes 1-9): Column existence, naming conventions
2. **Layer 0** (Codes 10-29): Global sanity checks (ranges, units)
3. **Layer 1-2** (Codes 30-49): Biomechanical envelopes
4. **Layer 3** (Codes 50-59): Physics consistency
5. **Layer 4** (Codes 60-69): Subject heuristics

### Enhanced Validation Tests (`enhanced_validation*.py`)
**Purpose**: Test advanced validation features

**Features Tested**:
- Comprehensive error collection
- Detailed reporting capabilities
- Export functionality (CSV, JSON)
- Performance optimization
- Custom validation rules

### Specification Compliance Tests (`spec_compliance*.py`)
**Purpose**: Ensure datasets meet standard specifications

**Compliance Areas**:
- Variable naming conventions
- Data format requirements
- Phase normalization (150 points)
- Unit consistency
- Metadata completeness

## Common Development Patterns

### Creating New Test Files
1. **Copy template structure** from existing test files
2. **Update module-specific details** (imports, class names, methods)
3. **Include comprehensive docstring** with creation date and purpose
4. **Add pytest fixtures** for common test data
5. **Implement manual test fallback** for environments without pytest
6. **Test all public methods** with multiple scenarios
7. **Include performance and edge case tests**

### Creating New Demo Files
1. **Copy template structure** from existing demo files
2. **Create realistic demonstration scenarios** with actual data
3. **Generate visual outputs** in appropriate sample_plots directory
4. **Include detailed explanations** for each demonstration
5. **Show integration patterns** with other modules
6. **Demonstrate error handling** and troubleshooting
7. **Provide usage examples** and best practices

### Managing Sample Plots
1. **Use consistent directory naming**: `sample_plots/demo_{module_name}/`
2. **Include descriptive file names** with scenario prefixes
3. **Clean up old plots** when updating demos
4. **Document plot contents** in demo script output
5. **Ensure plots are publication-ready** with proper labels and legends

## Testing Best Practices

### Code Quality
- **Comprehensive coverage**: Test all code paths and edge cases
- **Clear assertions**: Use descriptive error messages
- **Realistic data**: Use actual biomechanical patterns where possible
- **Performance awareness**: Monitor execution time and memory usage
- **Error handling**: Test both success and failure scenarios

### Documentation
- **Intent documentation**: Explain why tests exist and what they validate
- **Creation dates**: Include when tests were created for tracking
- **Usage examples**: Show how to run and interpret tests
- **Troubleshooting**: Include common issues and solutions
- **Integration guidance**: Show how tests fit into development workflow

### Maintenance
- **Regular updates**: Keep tests current with module changes
- **Performance monitoring**: Track test execution time trends
- **Data freshness**: Update test data to reflect current standards
- **Cleanup**: Remove obsolete tests and outdated plots
- **Documentation sync**: Keep test docs aligned with implementation

## Integration with Development Workflow

### Pre-commit Testing
- Run relevant test suites before committing changes
- Verify demo scripts generate expected outputs
- Check that new features have corresponding tests
- Ensure performance benchmarks are maintained

### Continuous Integration
- All test files should run without external dependencies
- Demo scripts should generate plots in predictable locations
- Tests should complete within reasonable time limits
- Visual outputs should be consistent and reproducible

### Development Guidelines
- **Test-driven development**: Write tests before implementing features
- **Demo-driven documentation**: Create demos that serve as living documentation
- **Visual validation**: Use plot generation to verify correctness
- **Performance tracking**: Monitor test execution time and memory usage

This testing framework ensures comprehensive validation of the locomotion data standardization system while providing educational resources for developers and clear visual demonstrations of functionality.