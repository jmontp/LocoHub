#!/usr/bin/env python3
"""
Manage Dataset Documentation

A tool for contributors to prepare and reset dataset documentation including
overview pages, validation reports, metadata, and plots.

Usage:
    python contributor_tools/manage_dataset_documentation.py add-dataset \
        --dataset converted_datasets/your_dataset_phase.parquet \
        [--metadata-file docs/datasets/_metadata/your_dataset.yaml] \
        [--ranges-file contributor_tools/validation_ranges/custom.yaml] \
        [--short-code UM21]
    python contributor_tools/manage_dataset_documentation.py update-documentation \
        --short-code UM21 [--dataset converted_datasets/your_dataset_phase.parquet] \
        [--metadata-file docs/datasets/_metadata/your_dataset.yaml]
    python contributor_tools/manage_dataset_documentation.py update-validation \
        --short-code UM21 [--dataset converted_datasets/your_dataset_phase.parquet] \
        [--ranges-file contributor_tools/validation_ranges/custom.yaml]
    python contributor_tools/manage_dataset_documentation.py remove-dataset \
        --short-code UM21 [--remove-parquet]

The tool will:
1. Collect dataset metadata (interactively or from --metadata-file)
2. Run validation and generate reports when requested
3. Create or refresh standardized documentation
4. Snapshot validation ranges for reproducibility
5. Package everything for PR submission
"""

import sys
import argparse
import yaml
import json
import re
import shutil
import subprocess
import gc
from pathlib import Path, PurePosixPath
from datetime import datetime
from typing import Dict, Optional, Tuple, List, Any
import os
import math
from collections import OrderedDict, defaultdict
from pandas import isna

# Add repository paths to sys.path so the package works without installation
current_dir = Path(__file__).parent
repo_root = current_dir.parent

# Ensure repo root is importable (for internal/* modules)
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

# Ensure src/ is importable so `import locohub` works without a venv
src_dir = repo_root / "src"
if src_dir.exists() and str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from locohub import LocomotionData

# Optional Dropbox SDK for share link generation
try:
    import dropbox
    from dropbox.sharing import SharedLinkSettings
    DROPBOX_SDK_AVAILABLE = True
except ImportError:
    DROPBOX_SDK_AVAILABLE = False

# Dropbox configuration environment variables
DROPBOX_FOLDER_ENV_VAR = "LOCOHUB_DROPBOX_FOLDER"
DROPBOX_TOKEN_ENV_VAR = "DROPBOX_ACCESS_TOKEN"

SITE_DATASET_BASE_URL = "https://jmontp.github.io/LocoHub/datasets"
DATASET_TABLE_FILES = [
    repo_root / "README.md",
    repo_root / "docs" / "index.md",
    repo_root / "docs" / "datasets" / "index.md",
]
TABLE_MARKER_START = "<!-- DATASET_TABLE_START -->"
TABLE_MARKER_END = "<!-- DATASET_TABLE_END -->"

VALIDATION_RANGES_DIR = repo_root / "contributor_tools" / "validation_ranges"
DEFAULT_RANGES_CANDIDATES = [
    VALIDATION_RANGES_DIR / "default_ranges.yaml",
]

DOCS_DATASETS_DIR = repo_root / "docs" / "datasets"
DOCS_DATASETS_GENERATED_DIR = DOCS_DATASETS_DIR / ".generated"


FEATURE_AVAILABILITY_STYLE = """
<style>
.feature-chip {display:inline-flex;align-items:center;justify-content:center;min-width:1.6rem;padding:0.1rem 0.55rem;border-radius:999px;font-weight:600;font-size:0.85rem;line-height:1;color:#ffffff;}
.feature-chip.feature-complete {background:#16a34a;}
.feature-chip.feature-partial {background:#facc15;color:#1f2937;}
.feature-chip.feature-missing {background:#ef4444;}
.feature-legend {margin-bottom:0.5rem;display:flex;gap:0.75rem;flex-wrap:wrap;}
.feature-legend .legend-item {display:flex;align-items:center;gap:0.35rem;font-size:0.9rem;}
.feature-source {font-size:0.85rem;color:#4b5563;margin:0.25rem 0 0.75rem 0;}
</style>
"""

FEATURE_STATUS_ICONS = {
    'complete': 'âœ”',
    'partial': 'â‰ˆ',
    'missing': 'âœ–',
}


def _render_feature_chip(status: str, coverage_pct: float) -> str:
    icon = FEATURE_STATUS_ICONS.get(status, 'â€“')
    try:
        pct_value = float(coverage_pct)
    except (TypeError, ValueError):
        pct_value = 0.0
    title = f"{pct_value:.1f}% available"
    pct_display = f"{pct_value:05.2f}" if status == 'partial' else ''
    label = f"{icon} {pct_display.strip()}" if pct_display else icon
    return f'<span class="feature-chip feature-{status}" title="{title}">{label}</span>'


def _resolve_default_ranges_file() -> Path:
    for candidate in DEFAULT_RANGES_CANDIDATES:
        if candidate.exists():
            return candidate
    # Return first candidate even if missing to preserve prior behaviour
    return DEFAULT_RANGES_CANDIDATES[0]


def _free_memory(message: str = "") -> None:
    """Explicitly free memory and run garbage collection."""
    gc.collect()
    if message:
        print(f"ðŸ§¹ {message}")


# =============================================================================
# Dropbox Integration Helpers
# =============================================================================

def _get_dropbox_folder(args) -> Optional[Path]:
    """Resolve Dropbox folder from command-line args or environment variable.

    Priority: --dropbox-folder arg > LOCOHUB_DROPBOX_FOLDER env var
    """
    folder_path = getattr(args, 'dropbox_folder', None)
    if not folder_path:
        folder_path = os.environ.get(DROPBOX_FOLDER_ENV_VAR)

    if not folder_path:
        print(f"âš ï¸  Dropbox folder not specified. Set {DROPBOX_FOLDER_ENV_VAR} or use --dropbox-folder")
        return None

    folder = Path(folder_path)
    if not folder.exists():
        print(f"âš ï¸  Dropbox folder does not exist: {folder}")
        print("   Creating folder...")
        try:
            folder.mkdir(parents=True, exist_ok=True)
        except Exception as exc:
            print(f"âŒ Failed to create Dropbox folder: {exc}")
            return None

    return folder


def _is_path_in_dropbox(path: Path, dropbox_folder: Path) -> bool:
    """Check if a path is already within the Dropbox folder."""
    try:
        path.resolve().relative_to(dropbox_folder.resolve())
        return True
    except ValueError:
        return False


def _copy_to_dropbox(source_path: Path, dropbox_folder: Path, short_code: str) -> Optional[Path]:
    """Copy a dataset file to the Dropbox folder.

    Files are organized as: {dropbox_folder}/{short_code}/{filename}
    If the file is already in Dropbox, returns its path without copying.
    """
    if not source_path.exists():
        print(f"âš ï¸  Source file not found: {source_path}")
        return None

    # Check if file is already in the Dropbox folder
    if _is_path_in_dropbox(source_path, dropbox_folder):
        print(f"   (already in Dropbox, skipping copy)")
        return source_path.resolve()

    dest_dir = dropbox_folder / short_code.lower()
    dest_dir.mkdir(parents=True, exist_ok=True)
    dest_path = dest_dir / source_path.name

    # Check if destination already exists and is the same file
    if dest_path.exists():
        if dest_path.stat().st_size == source_path.stat().st_size:
            print(f"   (already exists in Dropbox, skipping copy)")
            return dest_path

    try:
        shutil.copy2(source_path, dest_path)
        return dest_path
    except Exception as exc:
        print(f"âŒ Failed to copy to Dropbox: {exc}")
        return None


def _get_dropbox_api_path(local_dropbox_path: Path, dropbox_folder: Path) -> str:
    """Convert a local Dropbox path to the API path format.

    The API expects paths relative to the Dropbox root, starting with '/'.
    We need to figure out the Dropbox-relative path from the local filesystem path.
    """
    try:
        # Get the relative path from the dropbox folder
        rel_path = local_dropbox_path.relative_to(dropbox_folder)
        # Dropbox API paths start with / and use forward slashes
        api_path = "/" + rel_path.as_posix()
        return api_path
    except ValueError:
        # Path is not relative to dropbox_folder, return as-is
        return "/" + local_dropbox_path.name


def _generate_dropbox_share_link(
    dropbox_api_path: str,
    access_token: str,
    domain_restriction: Optional[str] = None
) -> Optional[str]:
    """Generate a Dropbox share link using the API.

    Args:
        dropbox_api_path: Path relative to Dropbox root (e.g., '/LocoHub/datasets/um21/file.parquet')
        access_token: Dropbox OAuth access token
        domain_restriction: Optional domain to restrict access (requires Business account)

    Returns:
        Share link URL or None if failed
    """
    if not DROPBOX_SDK_AVAILABLE:
        print("âš ï¸  Dropbox SDK not installed. Run: pip install dropbox")
        return None

    try:
        dbx = dropbox.Dropbox(access_token)

        # Configure sharing settings
        settings = None
        if domain_restriction:
            # Note: Domain-restricted sharing requires Dropbox Business
            # For personal accounts, this will be ignored or may fail
            try:
                settings = SharedLinkSettings(
                    requested_visibility=dropbox.sharing.RequestedVisibility.team_only
                )
                print(f"   Attempting team-only sharing (domain: {domain_restriction})")
            except Exception:
                print(f"âš ï¸  Domain restriction requires Dropbox Business account")
                settings = None

        # Try to create a new shared link
        try:
            if settings:
                link_metadata = dbx.sharing_create_shared_link_with_settings(
                    dropbox_api_path, settings=settings
                )
            else:
                link_metadata = dbx.sharing_create_shared_link_with_settings(dropbox_api_path)
            return link_metadata.url
        except dropbox.exceptions.ApiError as e:
            # Check if link already exists
            if hasattr(e.error, 'is_shared_link_already_exists') and e.error.is_shared_link_already_exists():
                # Get the existing link
                links = dbx.sharing_list_shared_links(path=dropbox_api_path, direct_only=True)
                if links.links:
                    return links.links[0].url
            raise

    except dropbox.exceptions.AuthError as e:
        print(f"âŒ Dropbox authentication failed: {e}")
        print(f"   Check your {DROPBOX_TOKEN_ENV_VAR} environment variable")
        return None
    except dropbox.exceptions.ApiError as e:
        print(f"âŒ Dropbox API error: {e}")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error generating share link: {e}")
        return None


def _handle_dropbox_upload(
    dataset_path: Path,
    short_code: str,
    args,
    metadata: Dict,
) -> None:
    """Handle Dropbox upload and share link generation for a dataset.

    This function:
    1. Copies clean and/or dirty dataset files to Dropbox
    2. Optionally generates share links via the API
    3. Updates metadata with download URLs
    """
    if not getattr(args, 'dropbox_upload', False):
        return

    dropbox_folder = _get_dropbox_folder(args)
    if not dropbox_folder:
        return

    generate_links = getattr(args, 'dropbox_share', False)
    access_token = os.environ.get(DROPBOX_TOKEN_ENV_VAR) if generate_links else None
    domain_restriction = getattr(args, 'dropbox_domain', None)

    if generate_links and not access_token:
        print(f"âš ï¸  Share link generation requested but {DROPBOX_TOKEN_ENV_VAR} not set")
        print("   Files will be copied but links won't be generated")
        generate_links = False

    if generate_links and not DROPBOX_SDK_AVAILABLE:
        print("âš ï¸  Dropbox SDK not installed. Run: pip install dropbox")
        print("   Files will be copied but links won't be generated")
        generate_links = False

    print(f"\nðŸ“¦ Uploading to Dropbox...")

    # Handle clean dataset
    clean_path, is_clean = _infer_clean_dataset_path(dataset_path)
    if clean_path.exists():
        print(f"   Copying clean dataset: {clean_path.name}")
        dropbox_clean_dest = _copy_to_dropbox(clean_path, dropbox_folder, short_code)
        if dropbox_clean_dest:
            print(f"   âœ… Copied to: {dropbox_clean_dest}")

            if generate_links and access_token:
                api_path = _get_dropbox_api_path(dropbox_clean_dest, dropbox_folder)
                print(f"   Generating share link for: {api_path}")
                link = _generate_dropbox_share_link(api_path, access_token, domain_restriction)
                if link:
                    # Convert to direct download link (dl=1 instead of dl=0)
                    if '?dl=0' in link:
                        link = link.replace('?dl=0', '?dl=1')
                    elif '?' not in link:
                        link = link + '?dl=1'
                    metadata['download_clean_url'] = link
                    print(f"   âœ… Share link: {link}")

    # Handle dirty dataset (if different from clean and exists)
    if '_dirty' in dataset_path.name or (not is_clean and dataset_path != clean_path):
        dirty_path = dataset_path if '_dirty' in dataset_path.name else None
        if not dirty_path:
            # Try to find dirty variant
            stem = dataset_path.stem
            if not stem.endswith('_dirty'):
                candidate = dataset_path.with_name(stem + '_dirty' + dataset_path.suffix)
                if candidate.exists():
                    dirty_path = candidate

        if dirty_path and dirty_path.exists() and dirty_path != clean_path:
            print(f"   Copying full/dirty dataset: {dirty_path.name}")
            dropbox_dirty_dest = _copy_to_dropbox(dirty_path, dropbox_folder, short_code)
            if dropbox_dirty_dest:
                print(f"   âœ… Copied to: {dropbox_dirty_dest}")

                if generate_links and access_token:
                    api_path = _get_dropbox_api_path(dropbox_dirty_dest, dropbox_folder)
                    print(f"   Generating share link for: {api_path}")
                    link = _generate_dropbox_share_link(api_path, access_token, domain_restriction)
                    if link:
                        if '?dl=0' in link:
                            link = link.replace('?dl=0', '?dl=1')
                        elif '?' not in link:
                            link = link + '?dl=1'
                        metadata['download_dirty_url'] = link
                        print(f"   âœ… Share link: {link}")


# =============================================================================
# End Dropbox Integration Helpers
# =============================================================================


def _resolve_ranges_argument(value: str) -> Path:
    """Resolve a user-supplied ranges argument to an absolute path."""

    candidate = Path(value)
    if candidate.is_absolute():
        return candidate

    search_roots = [
        repo_root,
        VALIDATION_RANGES_DIR,
        Path.cwd(),
    ]

    for root in search_roots:
        resolved = (root / candidate).resolve()
        if resolved.exists():
            return resolved

    # Fall back to treating it as relative to repo root (for error message consistency)
    return (repo_root / candidate).resolve()


def _infer_clean_dataset_path(dataset_path: Path) -> Tuple[Path, bool]:
    """Return the path to the clean dataset, falling back to the original."""

    stem = dataset_path.stem
    suffix = dataset_path.suffix

    # Already clean
    if stem.endswith('_clean'):
        return dataset_path, True

    candidates: List[Path] = []
    if stem.endswith('_dirty'):
        candidates.append(dataset_path.with_name(stem[:-6] + '_clean' + suffix))
    if stem.endswith('_raw'):
        candidates.append(dataset_path.with_name(stem[:-4] + '_clean' + suffix))

    candidates.append(dataset_path.with_name(stem + '_clean' + suffix))

    for candidate in candidates:
        if candidate.exists():
            return candidate, True

    return dataset_path, False


def _feature_group_for(feature_name: str) -> str:
    """Group feature names into broad categories for documentation tables."""

    lower = feature_name.lower()
    if 'grf' in lower:
        return 'Ground Reaction Forces'
    if 'power' in lower:
        return 'Joint Powers'
    if 'moment' in lower:
        return 'Joint Moments'
    if 'velocity' in lower:
        return 'Joint Velocities'
    if 'angle' in lower:
        return 'Joint Angles'
    return 'Other Features'


def _coverage_status(value: float) -> str:
    if value >= 0.99:
        return 'complete'
    if value <= 0.001:
        return 'missing'
    return 'partial'


def _compute_feature_task_groups(
    dataset_path: Path,
    tasks: List[str],
    locomotion_data: Optional[LocomotionData] = None,
) -> Tuple[Optional[List[Dict[str, Any]]], Optional[List[str]], Optional[Path]]:
    """Compute feature coverage per task using the clean dataset when available.

    Args:
        dataset_path: Path to the dataset file
        tasks: List of task names
        locomotion_data: Optional pre-loaded LocomotionData to reuse (avoids reload)
    """

    coverage_path, using_clean = _infer_clean_dataset_path(dataset_path)

    try:
        # Reuse pre-loaded data if provided and path matches
        if locomotion_data is not None and str(coverage_path) == str(dataset_path):
            coverage_data = locomotion_data
        else:
            coverage_data = LocomotionData(str(coverage_path), phase_col='phase_ipsi')
    except Exception as exc:
        print(f"âš ï¸  Unable to compute feature availability ({exc})")
        return None, None, None

    feature_columns = coverage_data.features or []
    if not feature_columns:
        return None, None, None

    df = coverage_data.df
    if 'task' not in df.columns:
        return None, None, None

    # Restrict to columns that actually exist in the DataFrame
    feature_columns = [col for col in feature_columns if col in df.columns]
    if not feature_columns:
        return None, None, None

    grouped = df.groupby('task')
    try:
        coverage = grouped[feature_columns].agg(lambda col: float(col.notna().mean()))
    except Exception as exc:
        print(f"âš ï¸  Unable to compute feature availability ({exc})")
        return None, None, None

    ordered_tasks: List[str] = []
    seen = set()
    for candidate in list(tasks) + list(coverage.index.astype(str)):
        if candidate not in seen:
            ordered_tasks.append(candidate)
            seen.add(candidate)

    grouped_features: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

    for feature in feature_columns:
        group_name = _feature_group_for(feature)
        task_entries: Dict[str, Any] = {}
        for task_name in ordered_tasks:
            value = 0.0
            if task_name in coverage.index and feature in coverage.columns:
                value = float(coverage.loc[task_name, feature])
                if math.isnan(value):
                    value = 0.0
            status = _coverage_status(value)
            task_entries[task_name] = {
                'status': status,
                'coverage': round(value * 100, 1),
            }
        grouped_features[group_name].append({
            'name': feature,
            'tasks': task_entries,
        })

    groups_output: List[Dict[str, Any]] = []
    for group_name in sorted(grouped_features.keys()):
        features_output = sorted(grouped_features[group_name], key=lambda item: item['name'])
        groups_output.append({
            'group': group_name,
            'features': features_output,
        })

    relative_path = coverage_path
    return groups_output, ordered_tasks, relative_path if using_clean else dataset_path


def _parse_subject_metadata_string(meta_str: str) -> Dict[str, str]:
    """Parse a subject_metadata comma-separated key:value string into a dict."""
    result: Dict[str, str] = {}
    if not meta_str:
        return result
    try:
        parts = [p.strip() for p in str(meta_str).split(',') if p and str(p).strip()]
        for p in parts:
            if ':' in p:
                k, v = p.split(':', 1)
                k = k.strip()
                v = v.strip()
                if k:
                    result[k] = v
    except Exception:
        # Best-effort parsing; ignore malformed segments
        pass
    return result


def _build_subject_metadata_table(locomotion_data: LocomotionData) -> Optional[str]:
    """Build a markdown table summarizing per-subject metadata if available.

    Returns a markdown string or None if the dataset has no subject_metadata column.
    """
    df = locomotion_data.df
    if 'subject_metadata' not in df.columns:
        return None

    # Collect first metadata string per subject
    subjects = sorted(locomotion_data.get_subjects())
    rows: List[Tuple[str, Dict[str, str]]] = []
    all_keys: List[str] = []
    seen_keys: set = set()

    preferred_order = [
        'age', 'sex', 'height_m', 'weight_kg',
        'leg_dominance', 'impairment', 'prosthesis_type', 'prosthesis_side', 'clinical_scores', 'notes'
    ]

    for subj in subjects:
        meta_series = df.loc[df['subject'] == subj, 'subject_metadata']
        meta_value = None
        if not meta_series.empty:
            # Take first non-null string
            for val in meta_series:
                if isinstance(val, str) and val.strip():
                    meta_value = val
                    break
        parsed = _parse_subject_metadata_string(meta_value or '')
        for k in parsed.keys():
            if k not in seen_keys:
                all_keys.append(k)
                seen_keys.add(k)
        rows.append((subj, parsed))

    if not seen_keys:
        return None

    # Order columns using preferred order, then any extras
    extra_keys = [k for k in all_keys if k not in preferred_order]
    ordered_keys = [k for k in preferred_order if k in seen_keys] + sorted(extra_keys)

    # Build markdown table
    header = ['Subject'] + [k for k in ordered_keys]
    lines = [
        '| ' + ' | '.join(header) + ' |',
        '|' + '|'.join(['---'] * len(header)) + '|',
    ]
    for subj, meta_dict in rows:
        cells = [subj]
        for k in ordered_keys:
            v = meta_dict.get(k, '')
            cells.append(str(v) if v is not None else '')
        lines.append('| ' + ' | '.join(cells) + ' |')

    return "\n".join(lines)
def _load_locomotion_data(dataset_path: Path, reason: str) -> LocomotionData:
    """Load dataset with a helpful progress message."""

    dataset_display = _relative_path(dataset_path)
    print(f"â³ Loading {dataset_display} ({reason})...")
    return LocomotionData(str(dataset_path), phase_col='phase_ipsi')


def _metadata_path_for_slug(slug: str) -> Path:
    return repo_root / "docs" / "datasets" / "_metadata" / f"{slug}.yaml"


def _load_metadata_for_slug(slug: str) -> Dict:
    metadata_path = _metadata_path_for_slug(slug)
    if not metadata_path.exists():
        raise FileNotFoundError(
            f"Metadata file not found for slug '{slug}'. Run add-dataset first."
        )
    metadata = load_metadata_file(metadata_path)
    metadata['dataset_name'] = slug
    metadata['short_code'] = metadata.get('short_code', slug).upper()
    return metadata


def _resolve_dataset_path(dataset_arg: Optional[str], metadata: Optional[Dict] = None) -> Path:
    """Resolve dataset path robustly.

    Strategy:
    - If an absolute path is provided and exists, use it.
    - For relative paths, try current working directory first, then repo root.
    - If just a filename, try Dropbox folder first (if configured), then converted_datasets/.
    - On failure, raise with a helpful message listing attempted locations.
    """

    # Build candidate strings from either argument or metadata
    raw_value: Optional[str] = None
    if dataset_arg:
        raw_value = str(dataset_arg)
    elif metadata and metadata.get('last_dataset_path'):
        raw_value = str(metadata['last_dataset_path'])
    else:
        raise ValueError("Dataset path is required; pass --dataset or add last_dataset_path to metadata.")

    provided = Path(raw_value)
    attempted: List[Path] = []

    # Absolute path case
    if provided.is_absolute():
        attempted.append(provided)
        if provided.exists():
            return provided.resolve()
    else:
        # Try relative to CWD first (user expectation), then repo root
        cwd_candidate = (Path.cwd() / provided).resolve()
        attempted.append(cwd_candidate)
        if cwd_candidate.exists():
            return cwd_candidate

        repo_candidate = (repo_root / provided).resolve()
        attempted.append(repo_candidate)
        if repo_candidate.exists():
            return repo_candidate

        # If user provided just a filename, try Dropbox first, then converted_datasets/
        if provided.name:
            # Try Dropbox folder first (if configured)
            dropbox_folder = os.environ.get(DROPBOX_FOLDER_ENV_VAR)
            if dropbox_folder:
                dropbox_candidate = (Path(dropbox_folder) / provided.name).resolve()
                attempted.append(dropbox_candidate)
                if dropbox_candidate.exists():
                    return dropbox_candidate

            # Fall back to local converted_datasets/
            cd_candidate = (repo_root / 'converted_datasets' / provided.name).resolve()
            attempted.append(cd_candidate)
            if cd_candidate.exists():
                return cd_candidate

    attempted_str = "\n  - ".join(str(p) for p in attempted)
    raise FileNotFoundError(
        "Dataset file not found. Tried:\n  - " + attempted_str
    )


def _prompt_metadata_updates(metadata: Dict) -> None:
    """Interactively prompt to update key metadata fields."""

    if metadata.get('download_clean_url') is None and metadata.get('download_url'):
        metadata['download_clean_url'] = metadata.get('download_url')
    metadata.setdefault('download_clean_url', '')
    metadata.setdefault('download_dirty_url', '')

    print("\nâœï¸ Update dataset details (press Enter to keep current values):")
    prompts = [
        ('display_name', 'Display name'),
        ('description', 'Dataset description'),
        ('institution', 'Institution or Lab'),
        ('year', 'Collection year'),
        ('download_clean_url', 'Clean dataset download URL (validated subset)'),
        ('download_dirty_url', 'Full dataset download URL (complete/dirty set)'),
        ('citation', 'Citation (DOI, BibTeX, or short reference)'),
        ('protocol', 'Collection protocol notes'),
        ('notes', 'Additional notes'),
    ]

    for key, label in prompts:
        current = metadata.get(key) or ''
        prompt = f"{label} [{current}]: " if current else f"{label}: "
        response = input(prompt).strip()
        if response:
            metadata[key] = response if key != 'year' else str(response)

    if not metadata.get('description'):
        metadata['description'] = 'Dataset description pending update.'


def _normalize_docs_link(path_value: Optional[str], slug: str, fallback_suffix: str) -> str:
    if not path_value:
        return f"./{slug}{fallback_suffix}"
    value = str(path_value)
    prefix = "docs/datasets/"
    if value.startswith(prefix):
        return "./" + value[len(prefix):]
    return value


def generate_comparison_snapshot() -> None:
    """Create a JSON snapshot for the comparison tool."""

    meta_dir = repo_root / "docs" / "datasets" / "_metadata"
    if not meta_dir.exists():
        return

    datasets_payload: List[Dict] = []

    for meta_file in sorted(meta_dir.glob('*.yaml')):
        data = load_metadata_file(meta_file)
        # Skip hidden datasets
        if data.get('hidden', False):
            continue
        slug = meta_file.stem

        comparison_tasks = data.get('comparison_tasks') or {}
        if isinstance(comparison_tasks, list):
            comparison_tasks = {entry.get('task'): entry for entry in comparison_tasks if isinstance(entry, dict) and entry.get('task')}

        plots_dir = repo_root / "docs" / "datasets" / "validation_plots" / slug

        task_entries: List[Dict] = []
        for task_name, task_info in sorted(comparison_tasks.items()):
            if not task_name:
                continue
            plots: List[str] = []
            if plots_dir.exists():
                for img_path in sorted(plots_dir.glob(f"*{task_name}*.png")):
                    rel_path = Path('validation_plots') / slug / img_path.name
                    plots.append(rel_path.as_posix())

            task_entries.append({
                'task': task_name,
                'display_name': _format_task_name(task_name),
                'pass_rate': task_info.get('pass_rate'),
                'status': task_info.get('status'),
                'total_strides': task_info.get('total_strides'),
                'failing_strides': task_info.get('failing_strides'),
                'plots': plots,
            })

        dataset_entry = {
            'slug': slug,
            'display_name': data.get('display_name', slug),
            'short_code': data.get('short_code', slug.upper()),
            'quality': data.get('quality_display') or data.get('validation_status') or 'â€”',
            'dataset_doc': _normalize_docs_link(data.get('doc_path'), slug, '.md'),
            'validation_report': _normalize_docs_link(data.get('validation_doc_path'), slug, '_validation.md'),
            'clean_url': data.get('download_clean_url') or data.get('download_url'),
            'dirty_url': data.get('download_dirty_url'),
            'tasks': task_entries,
        }

        datasets_payload.append(dataset_entry)

    snapshot_path = repo_root / "docs" / "datasets" / "comparison_data.json"
    snapshot_path.write_text(json.dumps({'datasets': datasets_payload}, indent=2), encoding='utf-8')

# Import validation modules
try:
    from contributor_tools.common.validation import Validator
except ImportError as e:
    print(f"âŒ Error importing validation modules: {e}")
    print("Make sure you're running from the repository root directory")
    sys.exit(1)


def check_existing_short_codes() -> Dict[str, str]:
    """
    Check for existing short codes in documentation.
    
    Returns:
        Dictionary mapping short codes to dataset names
    """
    codes = {}
    docs_dir = repo_root / "docs" / "datasets"
    
    if docs_dir.exists():
        for doc_file in docs_dir.glob("*.md"):
            try:
                with open(doc_file, 'r') as f:
                    content = f.read()
                    # Look for short code in frontmatter or metadata
                    match = re.search(r'short_code:\s*([A-Z0-9]+)', content)
                    if match:
                        codes[match.group(1)] = doc_file.stem
            except Exception:
                continue
    
    return codes


def _relative_path(path: Path) -> str:
    """Return repo-relative path if possible, else name."""
    try:
        return str(path.relative_to(repo_root))
    except ValueError:
        return path.name


def _display_path(path: Path) -> str:
    """Return repo-relative path if possible, else full path string."""
    try:
        return str(path.relative_to(repo_root))
    except ValueError:
        return str(path)


def load_metadata_file(path: Path) -> Dict:
    """Load metadata from a YAML or JSON file."""

    try:
        with open(path, "r") as f:
            if path.suffix.lower() in {".yml", ".yaml"}:
                data = yaml.safe_load(f)
            elif path.suffix.lower() == ".json":
                data = json.load(f)
            else:
                # Try YAML first, then JSON as fallback
                try:
                    data = yaml.safe_load(f)
                except yaml.YAMLError:
                    f.seek(0)
                    data = json.load(f)
    except Exception as exc:
        raise ValueError(f"Unable to load metadata file '{path}': {exc}") from exc

    if not isinstance(data, dict):
        raise ValueError("Metadata file must contain a mapping of keys to values")

    return data


def _normalize_tasks(value) -> List[str]:
    """Normalize tasks input to a list of strings."""

    if not value:
        return []

    if isinstance(value, str):
        return [task.strip() for task in value.split(",") if task.strip()]

    if isinstance(value, (list, tuple)):
        return [str(task).strip() for task in value if str(task).strip()]

    return []


def _format_task_name(task: str) -> str:
    return task.replace("_", " ").title()


def _slugify_short_code(short_code: str) -> str:
    return short_code.lower()


def _canonical_cell_value(value) -> str:
    """Return a stripped string representation, preserving blanks for hierarchy."""

    if isna(value):
        return ''
    return str(value).strip()


def _extract_task_catalog(locomotion_data: LocomotionData) -> Tuple[List[str], List[Tuple[str, str, str]]]:
    """Return sorted task list and hierarchical rows (task, task_id, task_info)."""

    detected_tasks = [task for task in locomotion_data.get_tasks() if task]
    required_cols = {'task', 'task_id', 'task_info'}
    if not required_cols.issubset(locomotion_data.df.columns):
        return detected_tasks, []

    subset = locomotion_data.df[['task', 'task_id', 'task_info']].drop_duplicates().copy()
    for column in ['task', 'task_id', 'task_info']:
        subset[column] = subset[column].apply(_canonical_cell_value)

    subset = subset.sort_values(['task', 'task_id', 'task_info'])
    rows = list(subset[['task', 'task_id', 'task_info']].itertuples(index=False, name=None))
    return detected_tasks, rows


def _build_task_table(rows: List[Tuple[str, str, str]]) -> str:
    """Build hierarchical markdown table from task rows."""

    if not rows:
        return ''

    lines = [
        "| Task | Task ID | Task Info |",
        "|------|---------|-----------|",
    ]
    prev_task: Optional[str] = None
    prev_task_id: Optional[str] = None

    for task, task_id, task_info in rows:
        task_value = task if task else 'â€”'
        task_id_value = task_id if task_id else 'â€”'
        task_info_value = task_info if task_info else 'â€”'

        if prev_task is not None and task == prev_task:
            task_cell = ' '
        else:
            task_cell = task_value

        if task != prev_task:
            prev_task_id = None

        if prev_task_id is not None and task_id == prev_task_id and task == prev_task:
            task_id_cell = ' '
        else:
            task_id_cell = task_id_value

        lines.append(f"| {task_cell} | {task_id_cell} | {task_info_value} |")

        prev_task = task
        prev_task_id = task_id

    return "\n".join(lines)


def write_metadata_file(metadata: Dict) -> None:
    dataset_name = metadata['dataset_name']
    meta_dir = repo_root / "docs" / "datasets" / "_metadata"
    meta_dir.mkdir(parents=True, exist_ok=True)
    meta_path = meta_dir / f"{dataset_name}.yaml"

    fields = OrderedDict()
    fields['display_name'] = metadata['display_name']
    fields['short_code'] = metadata['short_code']
    fields['description'] = metadata['description']
    fields['year'] = str(metadata['year'])
    fields['institution'] = metadata['institution']
    fields['subjects'] = str(metadata['subjects'])
    fields['tasks'] = metadata['tasks']
    if metadata.get('subject_metadata_table'):
        fields['subject_metadata_table'] = metadata.get('subject_metadata_table')
    if metadata.get('download_url'):
        fields['download_url'] = metadata['download_url']
    if metadata.get('download_clean_url'):
        fields['download_clean_url'] = metadata['download_clean_url']
    if metadata.get('download_dirty_url'):
        fields['download_dirty_url'] = metadata['download_dirty_url']
    if metadata.get('citation'):
        fields['citation'] = metadata['citation']
    if metadata.get('protocol'):
        fields['protocol'] = metadata['protocol']
    if metadata.get('notes'):
        fields['notes'] = metadata['notes']
    fields['date_added'] = metadata.get('date_added')
    fields['validation_status'] = metadata.get('validation_status')
    if metadata.get('validation_pass_rate') is not None:
        fields['validation_pass_rate'] = round(float(metadata['validation_pass_rate']), 1)
    fields['total_strides'] = metadata.get('validation_total_strides')
    fields['passing_strides'] = metadata.get('validation_passing_strides')
    fields['quality_display'] = metadata.get('quality_display')
    fields['doc_url'] = metadata.get('doc_url')
    fields['doc_path'] = metadata.get('doc_path')
    if metadata.get('doc_body_path'):
        fields['doc_body_path'] = metadata.get('doc_body_path')
    if metadata.get('validation_doc_url'):
        fields['validation_doc_url'] = metadata.get('validation_doc_url')
    if metadata.get('validation_doc_path'):
        fields['validation_doc_path'] = metadata.get('validation_doc_path')
    if metadata.get('validation_body_path'):
        fields['validation_body_path'] = metadata.get('validation_body_path')
    fields['validation_summary'] = metadata.get('validation_summary')
    if metadata.get('validation_ranges_file'):
        fields['validation_ranges_file'] = metadata.get('validation_ranges_file')
    if metadata.get('validation_ranges_download'):
        fields['validation_ranges_download'] = metadata.get('validation_ranges_download')
    if metadata.get('validation_ranges_source'):
        fields['validation_ranges_source'] = metadata.get('validation_ranges_source')
    if metadata.get('comparison_tasks'):
        fields['comparison_tasks'] = metadata.get('comparison_tasks')
    if metadata.get('feature_task_groups'):
        fields['feature_task_groups'] = metadata.get('feature_task_groups')
    if metadata.get('feature_task_order'):
        fields['feature_task_order'] = metadata.get('feature_task_order')
    if metadata.get('feature_task_source'):
        fields['feature_task_source'] = metadata.get('feature_task_source')
    if metadata.get('last_dataset_path'):
        fields['last_dataset_path'] = metadata.get('last_dataset_path')

    with open(meta_path, 'w') as fh:
        yaml.safe_dump(dict(fields), fh, sort_keys=False)


def _relative_link(path_value: Optional[str], table_file: Path) -> Optional[str]:
    if not path_value:
        return None
    anchor = ''
    target_str = str(path_value)
    if '#' in target_str:
        target_str, anchor = target_str.split('#', 1)
        anchor = f"#{anchor}" if anchor else ''

    if not target_str:
        return anchor or None

    target = Path(target_str)
    if not target.is_absolute():
        target = (repo_root / target).resolve()
    if target.suffix != '.md':
        target = target.with_suffix('.md')
    try:
        docs_root = repo_root / "docs"
        target_rel = target.relative_to(docs_root)
    except ValueError:
        # Path outside docs; return as-is (with anchor if present)
        return target.as_posix() + anchor

    table_rel = table_file.relative_to(docs_root)
    rel_path = PurePosixPath(
        os.path.relpath(target_rel.as_posix(), start=table_rel.parent.as_posix())
    )
    return rel_path.as_posix() + anchor


def _resolve_dataset_link(data: Dict, table_file: Path, absolute: bool) -> str:
    if absolute:
        return data.get('doc_url') or f"{SITE_DATASET_BASE_URL}/{data['dataset_name']}/"
    relative = _relative_link(data.get('doc_path'), table_file)
    if relative:
        if relative.endswith('.md'):
            relative = relative[:-3]
            if not relative.endswith('/'):
                relative += '/'
        return relative
    fallback = repo_root / "docs" / "datasets" / f"{data['dataset_name']}.md"
    rel_path = _relative_link(str(fallback), table_file)
    if rel_path and rel_path.endswith('.md'):
        rel_path = rel_path[:-3]
        if not rel_path.endswith('/'):
            rel_path += '/'
    return rel_path or f"{data['dataset_name']}/"


def _resolve_validation_link(data: Dict, table_file: Path, absolute: bool) -> Optional[str]:
    if absolute:
        return data.get('validation_doc_url') or f"{SITE_DATASET_BASE_URL}/{data['dataset_name']}_validation/"
    relative = _relative_link(data.get('validation_doc_path'), table_file)
    if relative:
        return relative
    fallback = repo_root / "docs" / "datasets" / f"{data['dataset_name']}_validation.md"
    return _relative_link(str(fallback), table_file)


def _build_dataset_table(metadata_entries: List[Dict], table_file: Path, absolute_links: bool) -> str:
    header = "| Dataset | Tasks | Documentation | Phase (Clean) | Phase | Time |"
    separator = "|---------|-------|---------------|---------------|-------|------|"

    rows: List[str] = []
    for data in sorted(metadata_entries, key=lambda d: d.get('display_name', '').lower()):
        doc_url = _resolve_dataset_link(data, table_file, absolute_links)
        display_name = data.get('display_name') or data['dataset_name']
        dataset_cell = display_name

        tasks_list = data.get('tasks', []) or []
        tasks_cell = ', '.join(_format_task_name(task) for task in tasks_list) if tasks_list else 'â€”'

        button_class_primary = 'md-button md-button--primary'
        button_class = 'md-button'

        if doc_url:
            doc_cell = f'<a class="{button_class_primary}" href="{doc_url}">Docs</a>'
        else:
            doc_cell = '<span class="md-button md-button--disabled">Docs</span>'

        # Phase (Clean) - validated/filtered dataset
        clean_url = data.get('download_clean_url') or data.get('download_url')
        if clean_url:
            phase_clean_cell = f'<a class="{button_class}" href="{clean_url}">Download</a>'
        else:
            phase_clean_cell = '<span class="md-button md-button--disabled">â€”</span>'

        # Phase - full/dirty dataset
        dirty_url = data.get('download_dirty_url')
        if dirty_url:
            phase_cell = f'<a class="{button_class}" href="{dirty_url}">Download</a>'
        else:
            phase_cell = '<span class="md-button md-button--disabled">â€”</span>'

        # Time data
        time_url = data.get('download_time_url')
        if time_url:
            time_cell = f'<a class="{button_class}" href="{time_url}">Download</a>'
        else:
            time_cell = '<span class="md-button md-button--disabled">â€”</span>'

        rows.append(
            "| "
            + " | ".join([
                dataset_cell,
                tasks_cell,
                doc_cell,
                phase_clean_cell,
                phase_cell,
                time_cell,
            ])
            + " |"
        )

    if not rows:
        empty_cells = ['_No datasets available_'] + [''] * 5
        return header + "\n" + separator + "\n| " + " | ".join(empty_cells) + " |"

    return "\n".join([header, separator] + rows)


def replace_between_markers(path: Path, content: str) -> None:
    text = path.read_text(encoding='utf-8')
    if TABLE_MARKER_START not in text or TABLE_MARKER_END not in text:
        return
    pattern = re.compile(
        rf"{re.escape(TABLE_MARKER_START)}.*?{re.escape(TABLE_MARKER_END)}",
        flags=re.DOTALL,
    )
    replacement = f"{TABLE_MARKER_START}\n{content}\n{TABLE_MARKER_END}"
    new_text = pattern.sub(lambda _: replacement, text, count=1)
    path.write_text(new_text, encoding='utf-8')


def update_dataset_tables() -> None:
    meta_dir = repo_root / "docs" / "datasets" / "_metadata"
    if not meta_dir.exists():
        return

    metadata_entries = []
    for meta_file in sorted(meta_dir.glob('*.yaml')):
        try:
            data = load_metadata_file(meta_file)
        except Exception as exc:
            print(f"âš ï¸ Skipping metadata file {meta_file.name}: {exc}")
            continue
        # Skip hidden datasets (e.g., AddBiomechanics datasets not yet ready for public)
        if data.get('hidden', False):
            continue
        data['dataset_name'] = meta_file.stem
        metadata_entries.append(data)

    if not metadata_entries:
        return

    for table_file in DATASET_TABLE_FILES:
        if table_file.exists():
            use_absolute = table_file.name.lower() == 'readme.md'
            table = _build_dataset_table(metadata_entries, table_file, use_absolute)
            replace_between_markers(table_file, table)


def handle_refresh_tables(args: argparse.Namespace) -> int:
    update_dataset_tables()
    print("âœ… Dataset tables refreshed.")
    return 0


def _remove_path(path: Path, removed: List[str]) -> None:
    """Delete files or directories that exist and track what changed."""

    if not path.exists() and not path.is_symlink():
        return

    try:
        if path.is_dir() and not path.is_symlink():
            shutil.rmtree(path)
        else:
            path.unlink()
        removed.append(str(path.relative_to(repo_root)))
    except Exception as exc:
        removed.append(f"FAILED:{path.relative_to(repo_root)} ({exc})")


def reset_dataset_assets(dataset_slug: str, include_parquet: bool = False) -> List[str]:
    """Remove generated assets for a dataset so it can be rebuilt."""

    removed: List[str] = []

    docs_dir = repo_root / "docs" / "datasets"
    metadata_dir = docs_dir / "_metadata"
    plots_dir = docs_dir / "validation_plots"

    _remove_path(docs_dir / f"{dataset_slug}.md", removed)
    _remove_path(docs_dir / f"{dataset_slug}_validation.md", removed)
    _remove_path(docs_dir / f"{dataset_slug}_validation_ranges.yaml", removed)
    _remove_path(metadata_dir / f"{dataset_slug}.yaml", removed)
    _remove_path(plots_dir / dataset_slug, removed)

    if plots_dir.exists():
        for file_path in plots_dir.glob(f"*{dataset_slug}*.png"):
            _remove_path(file_path, removed)

    checklist_path = repo_root / f"submission_checklist_{dataset_slug}.txt"
    _remove_path(checklist_path, removed)

    if include_parquet:
        for parquet_path in repo_root.glob(f"converted_datasets/{dataset_slug}*.parquet"):
            _remove_path(parquet_path, removed)

    try:
        update_dataset_tables()
    except Exception as exc:
        removed.append(f"FAILED:dataset_tables ({exc})")

    return removed


def _remove_dataset_for_slug(dataset_slug: str, include_parquet: bool = False) -> List[str]:
    """Wrapper to remove dataset assets and display summary."""

    try:
        removed_paths = reset_dataset_assets(dataset_slug, include_parquet=include_parquet)
    except Exception as exc:
        print(f"âŒ Failed to remove dataset assets: {exc}")
        return []
    if removed_paths:
        print(f"â™»ï¸ Removed existing dataset assets for '{dataset_slug}':")
        for path in removed_paths:
            print(f"  - {path}")
    else:
        print(f"â„¹ï¸ No existing assets found for '{dataset_slug}'.")
    return removed_paths


def _generate_validation_plots(dataset_path: Path, output_dir: Path, ranges_file: Optional[Path] = None) -> None:
    script_path = repo_root / 'contributor_tools' / 'quick_validation_check.py'
    if not script_path.exists():
        return
    cmd = [
        sys.executable,
        str(script_path),
        str(dataset_path),
        '--plot',
        '--output-dir',
        str(output_dir)
    ]
    if ranges_file:
        cmd.extend(['--ranges', str(ranges_file)])
    result = subprocess.run(cmd, cwd=repo_root)
    if result.returncode != 0:
        print('âš ï¸  Plot generation completed with non-zero exit code (plots saved for debugging).')


def update_validation_gallery(doc_path: Path, dataset_name: str) -> None:
    plots_dir = repo_root / 'docs' / 'datasets' / 'validation_plots' / dataset_name
    if not plots_dir.exists():
        return

    images = sorted(plots_dir.glob('*.png'))
    if not images:
        return

    variant_priority = {
        'clean': 4,
        'dirty': 3,
        'filtered': 2,
        '': 1,
        'raw': 0,
    }

    best_images: Dict[str, Dict[str, Any]] = {}
    allowed_tasks: Optional[set] = None

    for image_path in images:
        name = image_path.stem
        if '_phase_' not in name or '_all_features' not in name:
            continue
        segment = name.split('_phase_')[1].split('_all_features')[0]
        variant = ''
        task_segment = segment
        if '_' in segment:
            candidate, remainder = segment.split('_', 1)
            if candidate in variant_priority:
                variant = candidate
                task_segment = remainder
        task_segment = task_segment.replace('filtered_', '').replace('raw_', '')
        task_key = task_segment
        if allowed_tasks is not None and task_key not in allowed_tasks:
            continue
        priority = variant_priority.get(variant, 1)

        existing = best_images.get(task_key)
        if existing is None or priority > existing['priority']:
            best_images[task_key] = {
                'path': image_path,
                'priority': priority,
            }

    if not best_images:
        return

    meta_path = repo_root / 'docs' / 'datasets' / '_metadata' / f'{dataset_name}.yaml'
    ordered_tasks: List[str] = []
    if meta_path.exists():
        try:
            meta_data = load_metadata_file(meta_path)
            ordered_tasks = meta_data.get('tasks') or []
        except Exception:
            ordered_tasks = []

    allowed_tasks = set(ordered_tasks) if ordered_tasks else None
    if allowed_tasks is not None:
        best_images = {task: info for task, info in best_images.items() if task in allowed_tasks}
        if not best_images:
            return

    tabs_lines: List[str] = []

    seen = set()
    for task_name in ordered_tasks:
        if task_name in best_images:
            entry = best_images[task_name]
            rel_path = Path('validation_plots') / dataset_name / entry['path'].name
            task_title = _format_task_name(task_name)
            tabs_lines.append(f"=== \"{task_title}\"")
            tabs_lines.append(f"    ![{task_title}](./{rel_path.as_posix()})")
            tabs_lines.append("")
            seen.add(task_name)

    for task_name in sorted(best_images.keys()):
        if task_name in seen:
            continue
        entry = best_images[task_name]
        rel_path = Path('validation_plots') / dataset_name / entry['path'].name
        task_title = _format_task_name(task_name)
        tabs_lines.append(f"=== \"{task_title}\"")
        tabs_lines.append(f"    ![{task_title}](./{rel_path.as_posix()})")
        tabs_lines.append("")

    gallery = "\n".join(tabs_lines).strip() or "(Generate plots with quick_validation_check.py --plot)"

    # Wrap gallery with start/end markers for idempotent updates
    wrapped_gallery = f"<!-- VALIDATION_GALLERY_START -->\n{gallery}\n<!-- VALIDATION_GALLERY_END -->"

    content = doc_path.read_text(encoding='utf-8')

    # Check for existing gallery markers (idempotent replacement)
    gallery_pattern = re.compile(
        r'<!-- VALIDATION_GALLERY_START -->.*?<!-- VALIDATION_GALLERY_END -->',
        re.DOTALL
    )

    if gallery_pattern.search(content):
        # Replace existing gallery
        content = gallery_pattern.sub(wrapped_gallery, content)
    elif '<!-- VALIDATION_GALLERY -->' in content:
        # Replace legacy single marker
        content = content.replace('<!-- VALIDATION_GALLERY -->', wrapped_gallery)
    else:
        # No marker found - don't append (gallery should only exist where intended)
        pass

    doc_path.write_text(content, encoding='utf-8')


def _snapshot_validation_ranges(
    dataset_name: str,
    validation_doc_dir: Path,
    ranges_source_path: Path,
    ranges_display: str,
    validation_summary: str,
    metadata: Dict,
) -> Tuple[Optional[str], str]:
    validation_doc_dir.mkdir(parents=True, exist_ok=True)
    metadata['validation_ranges_source'] = _display_path(ranges_source_path)

    if not ranges_source_path.exists():
        print(f"âš ï¸  Validation ranges source not found: {ranges_source_path}")
        metadata.pop('validation_ranges_download', None)
        return None, validation_summary

    suffix = ranges_source_path.suffix or '.yaml'
    snapshot_path = validation_doc_dir / f"{dataset_name}_validation_ranges{suffix}"
    try:
        shutil.copyfile(ranges_source_path, snapshot_path)
    except Exception as exc:
        print(f"âš ï¸  Unable to snapshot validation ranges: {exc}")
        metadata.pop('validation_ranges_download', None)
        return None, validation_summary

    download_filename = snapshot_path.name
    metadata['validation_ranges_download'] = str(snapshot_path.relative_to(repo_root))
    download_md = f"[Download validation ranges](./{download_filename})"
    metadata['validation_ranges_file'] = download_md

    if ranges_display and ranges_display in validation_summary:
        validation_summary = validation_summary.replace(ranges_display, download_md)

    return download_filename, validation_summary


def generate_dataset_page(dataset_path: Path, metadata: Dict, validation_doc_filename: str) -> Tuple[Path, Path]:
    """Generate the dataset overview wrapper and documentation body."""

    dataset_slug = metadata['dataset_name']
    dataset_rel = _relative_path(dataset_path)
    date_added = metadata.get('date_added') or datetime.now().strftime('%Y-%m-%d')
    generated_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
    tasks_display = ', '.join(_format_task_name(task) for task in metadata['tasks'])

    task_catalog_section = ''
    task_table = metadata.get('task_table')
    if task_table:
        task_catalog_section = f"#### Task Catalog\n\n{task_table}\n"

    status_label = metadata.get('quality_display') or metadata.get('validation_status') or 'Validation pending'
    pass_rate = metadata.get('validation_pass_rate')
    pass_display = f"{float(pass_rate):.1f}%" if pass_rate is not None else 'â€”'

    validation_link = "#validation"
    ranges_entry = metadata.get('validation_ranges_file', 'â€”')
    ranges_source = metadata.get('validation_ranges_source')
    if ranges_source and ranges_entry != 'â€”':
        ranges_entry = f"{ranges_entry} (source: {ranges_source})"

    clean_url = (metadata.get('download_clean_url') or metadata.get('download_url') or '').strip()
    dirty_url = (metadata.get('download_dirty_url') or '').strip()

    description_text = metadata.get('description') or 'Dataset description pending update.'

    download_buttons: List[str] = []
    if clean_url:
        download_buttons.append(
            f'<a class="download-button available" href="{clean_url}" target="_blank" rel="noopener">Download Clean Dataset</a>'
        )
    else:
        download_buttons.append(
            '<span class="download-button unavailable" title="Clean dataset download not yet available">Clean Dataset (coming soon)</span>'
        )
    if dirty_url:
        download_buttons.append(
            f'<a class="download-button available" href="{dirty_url}" target="_blank" rel="noopener">Download Full Dataset (Dirty)</a>'
        )
    else:
        download_buttons.append(
            '<span class="download-button unavailable" title="Full dataset download not yet available">Full Dataset (coming soon)</span>'
        )

    buttons_html = "\n  ".join(download_buttons)
    download_note = ''
    if not clean_url and not dirty_url:
        download_note = '\n*Downloads coming soon. Contact the authors for data access.*'

    download_section = (
        "<style>\n"
        ".download-grid { display:flex; flex-wrap:wrap; gap:0.75rem; margin-bottom:1rem; }\n"
        ".download-button { display:inline-block; padding:0.65rem 1.4rem; border-radius:0.5rem; font-weight:600; text-decoration:none; }\n"
        ".download-button.available { background:#1f78d1; color:#fff; }\n"
        ".download-button.available:hover { background:#1663ad; }\n"
        ".download-button.unavailable { background:#d1d5db; color:#6b7280; cursor:not-allowed; }\n"
        "</style>\n"
        f"<div class=\"download-grid\">\n  {buttons_html}\n</div>"
        f"{download_note}"
    )

    doc_body_lines: List[str] = [
        "## Overview",
        "",
        f"- **Short Code**: {metadata['short_code']}",
        f"- **Year**: {metadata['year']}",
        f"- **Institution**: {metadata['institution']}",
        "",
        description_text,
        "",
        "## Downloads",
        "",
        download_section,
        "",
        "## Dataset Information",
        "",
        "### Subjects and Tasks",
        f"- **Number of Subjects**: {metadata['subjects']}",
        f"- **Tasks Included**: {tasks_display}",
        "",
    ]

    # Insert subject metadata table when available
    subj_meta_table = metadata.get('subject_metadata_table')
    if subj_meta_table:
        doc_body_lines.extend([
            "### Subject Metadata",
            "The table below summarizes the `subject_metadata` key:value pairs per subject.",
            subj_meta_table,
            "",
        ])

    if task_catalog_section:
        doc_body_lines.append(task_catalog_section.rstrip())
        doc_body_lines.append("")

    feature_groups_meta = metadata.get('feature_task_groups') or []
    if feature_groups_meta:
        task_order = metadata.get('feature_task_order') or metadata.get('tasks', [])
        if not task_order:
            gathered_tasks: List[str] = []
            for group_entry in feature_groups_meta:
                for feature_entry in group_entry.get('features', []):
                    gathered_tasks.extend(feature_entry.get('tasks', {}).keys())
            seen_tasks: set = set()
            task_order = []
            for task_name in gathered_tasks:
                if task_name not in seen_tasks:
                    task_order.append(task_name)
                    seen_tasks.add(task_name)

        formatted_tasks = [(task, _format_task_name(task)) for task in task_order]

        doc_body_lines.extend([
            "### Feature Availability by Task",
            FEATURE_AVAILABILITY_STYLE.strip(),
            "",
            '<div class="feature-legend">'
            '<span class="legend-item"><span class="feature-chip feature-complete">âœ”</span>Complete</span>'
            '<span class="legend-item"><span class="feature-chip feature-partial">â‰ˆ</span>Partial</span>'
            '<span class="legend-item"><span class="feature-chip feature-missing">âœ–</span>Missing</span>'
            '</div>',
        ])

        source_dataset = metadata.get('feature_task_source') or _relative_path(dataset_path)
        doc_body_lines.append(f'<p class="feature-source">Coverage computed from `{source_dataset}`.</p>')
        doc_body_lines.append("")

        for group_entry in feature_groups_meta:
            group_name = group_entry.get('group', 'Features')
            doc_body_lines.append(f"#### {group_name}")
            doc_body_lines.append("")

            header_cells = ["Feature"] + [display for _, display in formatted_tasks]
            header_line = "| " + " | ".join(header_cells) + " |"
            separator_line = "|" + "---|" * len(header_cells)
            doc_body_lines.append(header_line)
            doc_body_lines.append(separator_line)

            for feature_entry in group_entry.get('features', []):
                feature_name = feature_entry.get('name', '')
                task_statuses = feature_entry.get('tasks', {})
                row_cells = [f"`{feature_name}`"]
                for task_key, _ in formatted_tasks:
                    status_entry = task_statuses.get(task_key, {})
                    status = status_entry.get('status', 'missing')
                    coverage_pct = status_entry.get('coverage', 0.0)
                    row_cells.append(_render_feature_chip(status, coverage_pct))
                doc_body_lines.append("| " + " | ".join(row_cells) + " |")

            doc_body_lines.append("")

    doc_body_lines.extend([
        "### Data Structure",
        "- **Format**: Phase-normalized (150 points per gait cycle)",
        "- **Sampling**: Phase-indexed from 0-100%",
        "- **Variables**: Standard biomechanical naming convention",
        "",
        "## Validation Snapshot",
        "",
        f"- **Status**: {status_label}",
        f"- **Stride Pass Rate**: {pass_display}",
        f"- **Validation Ranges**: {ranges_entry}",
        f"- **Detailed Report**: [View validation report]({validation_link})",
        "",
        "## Citation",
        metadata.get('citation', 'Please cite appropriately when using this dataset.'),
        "",
        "## Collection Details",
        "",
        "### Protocol",
        metadata.get('protocol', 'Standard motion capture protocol was used.'),
        "",
        "### Processing Notes",
        metadata.get('notes', 'No additional notes.'),
        "",
        "## Files Included",
        "",
        f"- `{dataset_rel}` â€” Phase-normalized dataset",
        f"- [Validation report]({validation_link})",
        f"- Conversion script in `contributor_tools/conversion_scripts/{dataset_slug}/`",
        "",
        "---",
        "",
        f"*Generated by Dataset Submission Tool on {generated_timestamp}*",
    ])

    doc_body_content = "\n".join(line for line in doc_body_lines if line is not None) + "\n"

    DOCS_DATASETS_GENERATED_DIR.mkdir(parents=True, exist_ok=True)
    doc_body_path = DOCS_DATASETS_GENERATED_DIR / f"{dataset_slug}_documentation.md"
    doc_body_path.write_text(doc_body_content, encoding='utf-8')

    doc_snippet_target = (PurePosixPath('datasets') / doc_body_path.relative_to(DOCS_DATASETS_DIR)).as_posix()
    validation_snippet_target = (PurePosixPath('datasets') / Path(validation_doc_filename)).as_posix()

    wrapper_content = (
        f"---\n"
        f"title: {metadata['display_name']}\n"
        f"short_code: {metadata['short_code']}\n"
        f"date_added: {date_added}\n"
        f"---\n\n"
        f"# {metadata['display_name']}\n\n"
        "=== \"Documentation\"\n\n"
        f"    --8<-- \"{doc_snippet_target}\"\n\n"
        "=== \"Validation\"\n\n"
        f"    --8<-- \"{validation_snippet_target}\"\n"
    )

    DOCS_DATASETS_DIR.mkdir(parents=True, exist_ok=True)
    wrapper_path = DOCS_DATASETS_DIR / f"{dataset_slug}.md"
    wrapper_path.write_text(wrapper_content, encoding='utf-8')

    metadata['doc_body_path'] = str(doc_body_path.relative_to(repo_root))

    return wrapper_path, doc_body_path



def generate_validation_page(
    dataset_path: Path,
    metadata: Dict,
    validation_summary: str,
    validation_stats: Optional[Dict],
    validation_ranges: Dict[str, Dict[int, Dict[str, Dict[str, float]]]],
    validation_doc_path: Path,
    ranges_download_filename: Optional[str] = None,
    ranges_source_display: Optional[str] = None,
) -> Path:
    """Generate the per-dataset validation markdown body for tabbed layout."""

    generated_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
    pass_rate = metadata.get('validation_pass_rate')
    pass_display = f"{float(pass_rate):.1f}%" if pass_rate is not None else 'â€”'
    total_strides = (
        metadata.get('validation_total_strides')
        or metadata.get('total_strides')
        or 'â€”'
    )
    passing_strides = (
        metadata.get('validation_passing_strides')
        or metadata.get('passing_strides')
        or 'â€”'
    )

    stats_lines = [
        "| Metric | Value |",
        "|--------|-------|",
        f"| Stride Pass Rate | {pass_display} |",
        f"| Total Strides | {total_strides} |",
        f"| Passing Strides | {passing_strides} |",
    ]
    stats_table = "\n".join(stats_lines)

    if ranges_download_filename:
        ranges_section = (
            "Download the YAML snapshot used for this validation: "
            f"[Download](./{ranges_download_filename})"
        )
    else:
        ranges_section = "Validation ranges snapshot not available."

    if ranges_source_display:
        ranges_section += f"\n\n_Source ranges file: {ranges_source_display}_"

    validation_doc_path.parent.mkdir(parents=True, exist_ok=True)

    body_lines = [
        f"**Report generated:** {generated_timestamp}",
        "",
        "## Status Summary",
        "",
        stats_table,
        "",
        validation_summary.strip(),
        "",
        "## Validation Ranges Snapshot",
        "",
        ranges_section,
        "",
        "## Validation Plots",
        "",
        "<!-- VALIDATION_GALLERY -->",
        "",
        "---",
        "",
        f"*Generated from `{_relative_path(dataset_path)}` on {generated_timestamp}*",
    ]

    validation_content = "\n".join(body_lines) + "\n"
    validation_doc_path.write_text(validation_content, encoding='utf-8')

    metadata['validation_body_path'] = str(validation_doc_path.relative_to(repo_root))

    return validation_doc_path



def run_validation(dataset_path: Path, ranges_path: Optional[Path] = None, locomotion_data: Optional[LocomotionData] = None) -> Tuple[Dict, str, Optional[Dict], Dict[str, Dict[int, Dict[str, Dict[str, float]]]]]:
    """
    Run validation on the dataset.

    Args:
        dataset_path: Path to the dataset parquet file
        ranges_path: Optional path to validation ranges file
        locomotion_data: Optional pre-loaded LocomotionData to reuse (avoids reload)

    Returns:
        Tuple of (validation_result, summary_text, stats_dict, ranges_dict)
    """
    print(f"ðŸ” Running validation...")

    if ranges_path is not None:
        ranges_file = Path(ranges_path)
        if not ranges_file.is_absolute():
            ranges_file = (repo_root / ranges_file).resolve()
    else:
        ranges_file = _resolve_default_ranges_file()

    if not ranges_file.exists():
        print(f"âš ï¸  Validation ranges file not found: {ranges_file}")
        return {}, "Validation not run (ranges file missing)", None, {}

    ranges_display = _display_path(ranges_file)

    if ranges_path is not None:
        print(f"ðŸ”§ Using validation ranges from {ranges_display}")

    try:
        validator = Validator(config_path=ranges_file)
        validation_result = validator.validate(str(dataset_path))

        summary_data = validation_result.get('summary') or {}
        stats_block = validation_result.get('stats') or {}

        if not summary_data and stats_block:
            total = stats_block.get('total_strides', 0)
            failing = stats_block.get('total_failing_strides', 0)
            passed = total - failing
        elif summary_data:
            total = summary_data.get('total_strides', 0)
            passed = summary_data.get('passing_strides', 0)
        else:
            dataset_hint = _relative_path(dataset_path)
            fallback = (
                "âš ï¸ Validation summary unavailable from automated run.\n"
                f"Run `python contributor_tools/quick_validation_check.py {dataset_hint}` "
                "and paste the results here."
            )
            return validation_result, fallback, None, {}

        # Calculate pass rate
        pass_rate = (passed / total * 100) if total > 0 else 0

        # Generate summary text
        status = "âœ… PASSED" if pass_rate >= 95 else "âš ï¸ PARTIAL" if pass_rate >= 80 else "âŒ NEEDS REVIEW"

        # Reuse pre-loaded data or load fresh
        if locomotion_data is None:
            locomotion_data = _load_locomotion_data(dataset_path, "gathering per-task validation stats")
        task_stats = {}
        summary_rows = []
        for task in locomotion_data.get_tasks():
            task_data = locomotion_data.df[locomotion_data.df['task'] == task]
            total_cycles = len(task_data) // 150 if len(task_data) else 0
            failing = validator._validate_task_with_failing_features(locomotion_data, task, None)
            failing_strides = len(failing)
            task_pass_rate = 100.0 * (1 - failing_strides / total_cycles) if total_cycles else 0.0
            if task_pass_rate >= 90:
                task_status = "âœ…"
            elif task_pass_rate >= 70:
                task_status = "âš ï¸"
            else:
                task_status = "âŒ"
            task_stats[task] = {
                'pass_rate': task_pass_rate,
                'status': task_status,
                'total_strides': total_cycles,
                'failing_strides': failing_strides,
            }
            summary_rows.append(f"| {_format_task_name(task)} | {task_pass_rate:.1f}% | {task_status} |")

        summary = f"""### Summary

**Status**: {status} ({pass_rate:.1f}% valid)  
**Total Strides**: {total}  
**Passing Strides**: {passed}  

### Task Breakdown

| Task | Pass Rate | Status |
|------|-----------|--------|
""" + "\n".join(summary_rows) + "\n"

        summary += "\n_Validation ranges snapshot embedded below._\n"

        if pass_rate < 80:
            summary += "\nâš ï¸ **Note**: Validation ranges represent established biomechanical norms from literature. "
            summary += "Pass rates indicate conformance to these normsâ€”lower rates may reflect special populations, "
            summary += "non-standard protocols, or task-specific differences. See the [validation interpretation guide](../../contributor_tools/CLAUDE.md#interpreting-pass-rates) for details.\n"

        stats = {
            'total_strides': total,
            'passing_strides': passed,
            'pass_rate': pass_rate,
            'status': status,
            'tasks': task_stats,
            'ranges_path': str(ranges_file),
            'ranges_display': ranges_display,
        }

        # Capture validation ranges for tasks present in the dataset
        ranges_snapshot: Dict[str, Dict[int, Dict[str, Dict[str, float]]]] = {}
        for task in locomotion_data.get_tasks():
            task_ranges = validator.config_manager.get_task_data(task)
            if not task_ranges:
                continue
            organized: Dict[int, Dict[str, Dict[str, float]]] = {}
            for phase in sorted(task_ranges.keys()):
                phase_ranges = {}
                for var_name, bounds in task_ranges[phase].items():
                    if not isinstance(bounds, dict):
                        continue
                    phase_ranges[var_name] = {
                        'min': float(bounds.get('min')) if bounds.get('min') is not None else None,
                        'max': float(bounds.get('max')) if bounds.get('max') is not None else None,
                    }
                if phase_ranges:
                    organized[int(phase)] = phase_ranges
            if organized:
                ranges_snapshot[task] = organized

        return validation_result, summary, stats, ranges_snapshot

    except Exception as e:
        print(f"âš ï¸  Validation failed: {e}")
        return {}, f"âš ï¸ Validation could not be completed: {str(e)}", None, {}


def generate_submission_checklist(
    dataset_slug: str,
    dataset_file: Path,
    doc_wrapper_path: Path,
    doc_body_path: Path,
    validation_doc_path: Path,
) -> str:
    """Generate a checklist for the PR submission.

    Args:
        dataset_slug: Slugified dataset identifier
        dataset_file: Path to dataset parquet file
        doc_wrapper_path: Path to the tabbed documentation wrapper
        doc_body_path: Path to the documentation body snippet
        validation_doc_path: Path to the validation body snippet

    Returns:
        Formatted checklist string
    """
    dataset_file_rel = _relative_path(dataset_file)
    conversion_hint = f"contributor_tools/conversion_scripts/{dataset_slug}/"
    plots_hint = f"docs/datasets/validation_plots/{dataset_slug}/"

    checklist = f"""
ðŸ“‹ SUBMISSION CHECKLIST
{'='*60}

Your dataset submission is ready! Please include the following in your PR:

REQUIRED FILES:
â–¡ Dataset file: {dataset_file_rel}
â–¡ Documentation wrapper: {doc_wrapper_path.relative_to(repo_root)}
â–¡ Documentation body: {doc_body_path.relative_to(repo_root)}
â–¡ Validation body: {validation_doc_path.relative_to(repo_root)}
â–¡ Conversion script: {conversion_hint}

OPTIONAL FILES:
â–¡ Validation plots: {plots_hint}
â–¡ Custom validation ranges (if applicable)
â–¡ README with additional details

PR DESCRIPTION TEMPLATE:
------------------------
## New Dataset: {dataset_slug}

### Summary
[Brief description of the dataset]

### Validation Results
[Copy validation summary from above]

### Files Included
- Dataset parquet file
- Documentation
- Conversion script
- [Any additional files]

### Testing
- [ ] Validation passes with >80% rate
- [ ] Documentation is complete
- [ ] Conversion script is reproducible

### Notes
[Any special considerations or deviations from standard]
------------------------

NEXT STEPS:
1. Create a new branch for your changes
2. Add all files listed above
3. Commit with descriptive message
4. Push and create PR
5. Tag maintainers for review

Need help? Check the contributor guide or ask in discussions!
"""
    return checklist


def handle_add_dataset(args):
    """Handle the add-dataset command."""
    dataset_path = Path(args.dataset)

    # Validate dataset file
    if not dataset_path.exists():
        print(f"âŒ Dataset file not found: {dataset_path}")
        return 1

    dataset_path = dataset_path.resolve()

    custom_ranges_path: Optional[Path] = None
    if getattr(args, 'ranges_file', None):
        custom_ranges_path = _resolve_ranges_argument(args.ranges_file)
        if not custom_ranges_path.exists():
            print(f"âŒ Validation ranges file not found: {custom_ranges_path}")
            return 1

    if not dataset_path.suffix == '.parquet':
        print(f"âŒ Dataset must be a parquet file, got: {dataset_path.suffix}")
        return 1

    # Derive a fallback name from the parquet filename
    dataset_filename_stem = dataset_path.stem.replace("_phase", "").replace("_time", "")
    display_name = dataset_filename_stem.replace("_", " ").title()

    print(f"ðŸ“¦ Preparing submission for: {display_name}")
    print(f"{'='*60}")

    metadata_source: Optional[Dict] = None
    if getattr(args, 'metadata_file', None):
        metadata_path = Path(args.metadata_file)
        if not metadata_path.exists():
            print(f"âŒ Metadata file not found: {metadata_path}")
            return 1

        try:
            metadata_source = load_metadata_file(metadata_path)
        except ValueError as exc:
            print(f"âŒ {exc}")
            return 1

    # Load data ONCE and reuse throughout the function
    try:
        locomotion_data = _load_locomotion_data(dataset_path, "extracting tasks and subjects")
    except Exception as exc:
        print(f"âŒ Unable to load dataset for task extraction: {exc}")
        return 1

    detected_tasks, task_rows = _extract_task_catalog(locomotion_data)
    task_table = _build_task_table(task_rows)
    subject_count = len(locomotion_data.get_subjects())
    subjects_value = str(subject_count)
    # Build subject metadata table if available
    try:
        subject_metadata_table = _build_subject_metadata_table(locomotion_data)
    except Exception:
        subject_metadata_table = None

    non_interactive = metadata_source is not None
    preset_short_code = (getattr(args, 'short_code', None) or '').strip().upper() or None

    print(f"\nðŸ“ Dataset Metadata")
    if non_interactive:
        print("(loaded from metadata file)")
    else:
        print(f"{'='*40}")
        print("Please provide information about your dataset:\n")

    # Collect metadata (either from file or interactively)
    date_added = None
    short_code: Optional[str] = None
    dataset_slug: Optional[str] = None
    tasks: List[str] = detected_tasks.copy()
    download_clean_url = ''
    download_dirty_url = ''
    download_url = ''
    citation = None
    protocol = None
    notes = None

    if non_interactive:
        existing_codes = check_existing_short_codes()
        display_name = metadata_source.get('display_name', display_name)
        metadata_short_code = metadata_source.get('short_code')
        if metadata_short_code and preset_short_code and metadata_short_code.upper() != preset_short_code:
            print("âŒ Metadata file short_code does not match --short-code value")
            return 1
        short_code = (metadata_short_code or preset_short_code)
        if not short_code:
            print("âŒ Provide a short code via metadata file or --short-code")
            return 1
        short_code = short_code.upper()

        if not re.match(r'^[A-Z]{2}\d{2}[A-Z]?$', short_code):
            print("âŒ Short code must be 2 letters + 2 digits (optional trailing letter, e.g., UM21 or UM21F)")
            return 1

        candidate_slug = _slugify_short_code(short_code)
        existing_slug = existing_codes.get(short_code)
        if existing_slug and existing_slug != candidate_slug:
            print(f"âŒ Short code '{short_code}' already used by dataset '{existing_slug}'. Run remove-dataset --short-code {short_code} first.")
            return 1
        if existing_slug == candidate_slug:
            print(f"âŒ Dataset '{candidate_slug}' already exists. Use update commands or remove it before re-adding.")
            return 1
        dataset_slug = candidate_slug

        description = metadata_source.get('description', f"Biomechanical dataset from {display_name}")
        year = str(metadata_source.get('year', datetime.now().year))
        institution = metadata_source.get('institution', "[Please add institution name]")
        metadata_subjects = metadata_source.get('subjects')
        if metadata_subjects and str(metadata_subjects).strip() != subjects_value:
            print("â„¹ï¸ Detected subject count differs from metadata file; using dataset-derived count.")
        metadata_tasks = _normalize_tasks(metadata_source.get('tasks'))
        if tasks:
            if metadata_tasks and metadata_tasks != tasks:
                print("â„¹ï¸ Detected dataset tasks differ from metadata file; using dataset-derived list.")
        else:
            tasks = metadata_tasks
        if not tasks:
            tasks = ["[Please list tasks]"]

        download_clean_url = metadata_source.get('download_clean_url') or metadata_source.get('download_url') or ''
        download_dirty_url = metadata_source.get('download_dirty_url') or ''
        download_url = metadata_source.get('download_url') or ''
        citation = metadata_source.get('citation')
        protocol = metadata_source.get('protocol')
        notes = metadata_source.get('notes')
        date_added = metadata_source.get('date_added')
    else:
        try:
            # Short code validation
            existing_codes = check_existing_short_codes()
            default_stub = (display_name[:2].upper() or dataset_filename_stem[:2].upper() or "DS")
            while True:
                suggested_code = f"{default_stub}{str(datetime.now().year)[2:]}"
                if preset_short_code:
                    short_code = preset_short_code
                    preset_short_code = None
                    print(f"â„¹ï¸ Using provided short code '{short_code}'")
                else:
                    short_code_input = input(f"Short code (AA00 or AA00F) [{suggested_code}]: ").strip().upper()
                    short_code = short_code_input or suggested_code

                if not re.match(r'^[A-Z]{2}\d{2}[A-Z]?$', short_code):
                    print("âŒ Short code must be 2 letters + 2 digits (optional trailing letter, e.g., UM21 or UM21F)")
                    continue

                candidate_slug = _slugify_short_code(short_code)
                existing_slug = existing_codes.get(short_code)
                if existing_slug and existing_slug != candidate_slug:
                    print(f"âŒ Short code '{short_code}' is already assigned to dataset '{existing_slug}'. Run remove-dataset --short-code {short_code} first or choose another code.")
                    continue
                if existing_slug == candidate_slug:
                    print(f"âŒ Dataset '{candidate_slug}' already exists. Use update-documentation/update-validation or remove it before re-adding.")
                    continue

                print(f"âœ… Short code '{short_code}' accepted")
                dataset_slug = candidate_slug
                break

            # Basic metadata
            display_name = input(f"Display name [{display_name}]: ").strip() or display_name

            # Dataset details
            print("\nDataset Details:")
            description = input("Brief description (1-2 sentences): ").strip()
            if not description:
                description = f"Biomechanical dataset from {display_name}"

            year = input(f"Collection year [{datetime.now().year}]: ").strip()
            year = year or str(datetime.now().year)

            institution = input("Institution/Lab name: ").strip()
            if not institution:
                institution = "[Please add institution name]"

            print(f"\nDetected {subject_count} unique subjects in the dataset.")

            if tasks:
                print("\nDetected tasks from dataset:")
                for task_name in tasks:
                    print(f"  - {task_name}")
                if task_table:
                    print("\nDetected task catalog:")
                    print(task_table)
            else:
                print("\nTasks included (comma-separated):")
                print("  Common: level_walking, incline_walking, stair_ascent, stair_descent")
                tasks_input = input("Tasks: ").strip()
                tasks = [t.strip() for t in tasks_input.split(",")] if tasks_input else ["[Please list tasks]"]

            # Optional information
            print("\nOptional Information (press Enter to skip):")
            download_clean_url = input("Clean dataset download URL (validated subset): ").strip()
            download_dirty_url = input("Full dataset download URL (complete/dirty set): ").strip()
            download_url = input("Legacy single download URL (optional fallback): ").strip()
            citation = input("Citation (DOI, BibTeX, or short reference): ").strip()
            protocol = input("Collection protocol notes: ").strip()
            notes = input("Additional notes: ").strip()

            date_added = datetime.now().strftime('%Y-%m-%d')

        except KeyboardInterrupt:
            print("\n\nðŸ›‘ Submission preparation cancelled")
            return 1

    if not short_code:
        print("âŒ Short code was not provided")
        return 1

    if not dataset_slug:
        dataset_slug = _slugify_short_code(short_code)

    tasks = [task for task in tasks if task] or ["[Please list tasks]"]

    feature_task_groups: Optional[List[Dict[str, Any]]] = None
    feature_task_order: Optional[List[str]] = None
    feature_task_source: Optional[str] = None

    # Compute feature coverage (reuse loaded data to save memory)
    coverage_groups, coverage_tasks, coverage_source_path = _compute_feature_task_groups(
        dataset_path, tasks, locomotion_data=locomotion_data
    )
    if coverage_groups:
        feature_task_groups = coverage_groups
        if coverage_tasks:
            feature_task_order = coverage_tasks
        if coverage_source_path:
            feature_task_source = _relative_path(coverage_source_path)
    _free_memory()  # Clean up after feature coverage computation

    # Prepare metadata dictionary
    dataset_name = dataset_slug

    doc_path = DOCS_DATASETS_DIR / f"{dataset_name}.md"
    validation_doc_filename = f".generated/{dataset_name}_validation.md"

    validation_doc_path = DOCS_DATASETS_DIR / validation_doc_filename
    metadata_path = _metadata_path_for_slug(dataset_slug)
    ranges_snapshot_path = repo_root / "docs" / "datasets" / f"{dataset_name}_validation_ranges.yaml"

    if doc_path.exists() or validation_doc_path.exists() or metadata_path.exists() or ranges_snapshot_path.exists():
        print(
            f"âŒ Dataset assets already exist for '{dataset_slug}'. "
            "Use update-documentation/update-validation or run remove-dataset "
            f"--short-code {short_code} before re-adding."
        )
        return 1

    metadata = {
        'dataset_name': dataset_name,
        'display_name': display_name,
        'short_code': short_code,
        'description': description,
        'year': year,
        'institution': institution,
        'subjects': subjects_value,
        'tasks': tasks,
        'download_url': download_clean_url or download_url or None,
        'download_clean_url': download_clean_url if download_clean_url else None,
        'download_dirty_url': download_dirty_url if download_dirty_url else None,
        'citation': citation if citation else None,
        'protocol': protocol if protocol else None,
        'notes': notes if notes else None,
        'date_added': date_added or datetime.now().strftime('%Y-%m-%d'),
        'task_table': task_table,
        'last_dataset_path': _relative_path(dataset_path),
    }

    if feature_task_groups:
        metadata['feature_task_groups'] = feature_task_groups
    if feature_task_order:
        metadata['feature_task_order'] = feature_task_order
    if feature_task_source:
        metadata['feature_task_source'] = feature_task_source

    # Run validation (reuse loaded data to save memory)
    print(f"\nðŸ” Validating dataset...")
    _, validation_summary, validation_stats, validation_ranges = run_validation(
        dataset_path, custom_ranges_path, locomotion_data=locomotion_data
    )
    _free_memory()  # Clean up after validation
    metadata['validation_summary'] = validation_summary
    metadata['doc_url'] = f"{SITE_DATASET_BASE_URL}/{dataset_name}/"
    metadata['doc_path'] = f"docs/datasets/{dataset_name}.md"
    metadata['validation_doc_url'] = f"{SITE_DATASET_BASE_URL}/{dataset_name}/#validation"
    metadata['validation_doc_path'] = f"docs/datasets/{dataset_name}.md#validation"

    ranges_display = _display_path(custom_ranges_path) if custom_ranges_path else _display_path(_resolve_default_ranges_file())
    plot_ranges_path: Optional[Path] = custom_ranges_path

    if validation_stats:
        metadata['validation_status'] = validation_stats.get('status')
        metadata['validation_pass_rate'] = validation_stats.get('pass_rate')
        metadata['validation_total_strides'] = validation_stats.get('total_strides')
        metadata['validation_passing_strides'] = validation_stats.get('passing_strides')
        if validation_stats.get('ranges_display'):
            ranges_display = validation_stats['ranges_display']
        ranges_path_str = validation_stats.get('ranges_path')
        if ranges_path_str:
            resolved = Path(ranges_path_str)
            if not resolved.is_absolute():
                resolved = (repo_root / resolved).resolve()
            plot_ranges_path = resolved
        status_text = validation_stats.get('status', '')
        pass_rate = validation_stats.get('pass_rate') or 0
        if 'PASSED' in status_text:
            metadata['quality_display'] = 'âœ… Validated'
        elif 'PARTIAL' in status_text:
            metadata['quality_display'] = f"âš ï¸ Partial ({pass_rate:.1f}%)"
        elif 'NEEDS REVIEW' in status_text:
            metadata['quality_display'] = f"âŒ Needs Review ({pass_rate:.1f}%)"
        else:
            metadata['quality_display'] = status_text or 'â€”'
        metadata['comparison_tasks'] = validation_stats.get('tasks')
    else:
        metadata['validation_status'] = 'UNKNOWN'
        metadata['validation_pass_rate'] = None
        metadata['validation_total_strides'] = None
        metadata['validation_passing_strides'] = None
        metadata['quality_display'] = 'âš ï¸ Validation Pending'
        metadata['comparison_tasks'] = metadata.get('comparison_tasks') or {}
    metadata['validation_ranges_file'] = ranges_display

    if plot_ranges_path is None:
        plot_ranges_path = _resolve_default_ranges_file()

    ranges_source_path = Path(plot_ranges_path)
    if not ranges_source_path.is_absolute():
        ranges_source_path = (repo_root / ranges_source_path).resolve()

    validation_doc_path = DOCS_DATASETS_DIR / validation_doc_filename

    ranges_download_filename, validation_summary = _snapshot_validation_ranges(
        dataset_name,
        DOCS_DATASETS_DIR,
        ranges_source_path,
        ranges_display,
        validation_summary,
        metadata,
    )
    metadata['validation_summary'] = validation_summary
    ranges_source_display = metadata.get('validation_ranges_source')

    # Show validation results
    print(f"\nðŸ“Š Validation Results:")
    print(validation_summary)

    # Handle Dropbox upload (before generating docs so URLs are included)
    _handle_dropbox_upload(dataset_path, short_code, args, metadata)

    # Generate documentation
    print(f"\nðŸ“„ Generating documentation...")
    doc_wrapper_path, doc_body_path = generate_dataset_page(dataset_path, metadata, validation_doc_filename)
    validation_body_path = generate_validation_page(
        dataset_path,
        metadata,
        validation_summary,
        validation_stats,
        validation_ranges,
        validation_doc_path,
        ranges_download_filename,
        ranges_source_display,
    )
    print(f"âœ… Tab wrapper created: {doc_wrapper_path.relative_to(repo_root)}")
    print(f"âœ… Documentation body created: {doc_body_path.relative_to(repo_root)}")
    print(f"âœ… Validation body created: {validation_body_path.relative_to(repo_root)}")

    # Generate plots directory structure
    plots_dir = repo_root / "docs" / "datasets" / "validation_plots" / dataset_name
    plots_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Plot directory prepared: {plots_dir.relative_to(repo_root)}/")

    # Free memory before spawning plot subprocess (it will load data again)
    del locomotion_data
    _free_memory("Freeing memory before plot generation...")

    # Generate fresh validation plots for the dataset
    try:
        plot_ranges_for_cmd = None
        if plot_ranges_path and Path(plot_ranges_path).exists():
            plot_ranges_for_cmd = Path(plot_ranges_path)
        _generate_validation_plots(dataset_path, plots_dir, plot_ranges_for_cmd)
    except subprocess.CalledProcessError as exc:
        print(f"âš ï¸  Plot generation failed: {exc}")

    update_validation_gallery(validation_body_path, dataset_name)

    # Persist metadata and refresh global tables
    write_metadata_file(metadata)
    update_dataset_tables()
    generate_comparison_snapshot()

    
    # Show submission checklist
    checklist = generate_submission_checklist(
        dataset_name,
        dataset_path,
        doc_wrapper_path,
        doc_body_path,
        validation_body_path,
    )
    print(checklist)
    
    # Save checklist to file
    checklist_path = repo_root / f"submission_checklist_{dataset_name}.txt"
    with open(checklist_path, 'w') as f:
        f.write(checklist)
    print(f"ðŸ’¾ Checklist saved to: {checklist_path.name}")
    
    print(f"\nðŸŽ‰ SUCCESS! Your dataset submission is prepared!")
    print(f"   Follow the checklist above to complete your PR")
    
    return 0





def handle_update_documentation(args):
    """Regenerate the dataset overview page and metadata."""
    short_code = args.short_code.upper()
    if not re.match(r'^[A-Z]{2}\d{2}[A-Z]?$', short_code):
        print("âŒ Short code must be 2 letters + 2 digits (optional trailing letter)")
        return 1

    dataset_slug = _slugify_short_code(short_code)
    try:
        metadata = _load_metadata_for_slug(dataset_slug)
    except FileNotFoundError as exc:
        print(f"âŒ {exc}")
        return 1

    dataset_arg = getattr(args, 'dataset', None)
    try:
        dataset_path = _resolve_dataset_path(dataset_arg, metadata)
    except (ValueError, FileNotFoundError) as exc:
        print(f"âŒ {exc}")
        return 1

    metadata['short_code'] = short_code
    metadata['dataset_name'] = dataset_slug
    metadata['doc_url'] = metadata.get('doc_url') or f"{SITE_DATASET_BASE_URL}/{dataset_slug}/"
    metadata['doc_path'] = metadata.get('doc_path') or f"docs/datasets/{dataset_slug}.md"
    metadata['validation_doc_url'] = metadata.get('validation_doc_url') or f"{SITE_DATASET_BASE_URL}/{dataset_slug}/#validation"
    metadata['validation_doc_path'] = metadata.get('validation_doc_path') or f"docs/datasets/{dataset_slug}.md#validation"
    if metadata.get('validation_doc_path', '').endswith('_validation.md'):
        metadata['validation_doc_path'] = f"docs/datasets/{dataset_slug}.md#validation"
    if metadata.get('validation_doc_url', '').endswith('_validation/'):
        metadata['validation_doc_url'] = f"{SITE_DATASET_BASE_URL}/{dataset_slug}/#validation"
    metadata['last_dataset_path'] = _relative_path(dataset_path)

    override_path = getattr(args, 'metadata_file', None)
    if override_path:
        override_path = Path(override_path)
        if not override_path.exists():
            print(f"âŒ Metadata file not found: {override_path}")
            return 1
        try:
            overrides = load_metadata_file(override_path)
        except ValueError as exc:
            print(f"âŒ {exc}")
            return 1
        override_code = overrides.get('short_code')
        if override_code and override_code.upper() != short_code:
            print("âŒ Metadata file short_code does not match --short-code")
            return 1
        metadata.update({k: v for k, v in overrides.items() if v is not None})
        metadata['short_code'] = short_code
        metadata['dataset_name'] = dataset_slug

    try:
        locomotion_data = _load_locomotion_data(dataset_path, "refreshing tasks and subjects")
    except Exception as exc:
        print(f"âŒ Unable to load dataset: {exc}")
        return 1

    detected_tasks, task_rows = _extract_task_catalog(locomotion_data)
    if detected_tasks:
        metadata['tasks'] = detected_tasks
    else:
        metadata['tasks'] = metadata.get('tasks', [])
    metadata['subjects'] = str(len(locomotion_data.get_subjects()))
    metadata['task_table'] = _build_task_table(task_rows)
    # Subject metadata table
    try:
        subject_metadata_table = _build_subject_metadata_table(locomotion_data)
        if subject_metadata_table:
            metadata['subject_metadata_table'] = subject_metadata_table
        else:
            metadata.pop('subject_metadata_table', None)
    except Exception:
        metadata.pop('subject_metadata_table', None)
    # Subject metadata table
    try:
        subject_metadata_table = _build_subject_metadata_table(locomotion_data)
        if subject_metadata_table:
            metadata['subject_metadata_table'] = subject_metadata_table
        else:
            metadata.pop('subject_metadata_table', None)
    except Exception:
        metadata.pop('subject_metadata_table', None)

    coverage_groups, coverage_tasks, coverage_source_path = _compute_feature_task_groups(dataset_path, metadata['tasks'])
    if coverage_groups:
        metadata['feature_task_groups'] = coverage_groups
        if coverage_tasks:
            metadata['feature_task_order'] = coverage_tasks
        if coverage_source_path:
            metadata['feature_task_source'] = _relative_path(coverage_source_path)

    if not override_path:
        _prompt_metadata_updates(metadata)

    # Persist subject metadata table in metadata for rendering
    if subject_metadata_table:
        metadata['subject_metadata_table'] = subject_metadata_table

    # Handle Dropbox upload (before generating docs so URLs are included)
    _handle_dropbox_upload(dataset_path, short_code, args, metadata)

    validation_doc_filename = f".generated/{dataset_slug}_validation.md"
    doc_path, doc_body_path = generate_dataset_page(dataset_path, metadata, validation_doc_filename)
    write_metadata_file(metadata)
    update_dataset_tables()
    generate_comparison_snapshot()

    print(f"âœ… Tab wrapper refreshed: {doc_path.relative_to(repo_root)}")
    print(f"âœ… Documentation body refreshed: {doc_body_path.relative_to(repo_root)}")
    return 0


def handle_update_validation(args):
    """Re-run validation and refresh the validation report."""
    short_code = args.short_code.upper()
    if not re.match(r'^[A-Z]{2}\d{2}[A-Z]?$', short_code):
        print("âŒ Short code must be 2 letters + 2 digits (optional trailing letter)")
        return 1

    dataset_slug = _slugify_short_code(short_code)
    try:
        metadata = _load_metadata_for_slug(dataset_slug)
    except FileNotFoundError as exc:
        print(f"âŒ {exc}")
        return 1

    dataset_arg = getattr(args, 'dataset', None)
    try:
        dataset_path = _resolve_dataset_path(dataset_arg, metadata)
    except (ValueError, FileNotFoundError) as exc:
        print(f"âŒ {exc}")
        return 1

    custom_ranges_path: Optional[Path] = None
    if getattr(args, 'ranges_file', None):
        custom_ranges_path = _resolve_ranges_argument(args.ranges_file)
        if not custom_ranges_path.exists():
            print(f"âŒ Validation ranges file not found: {custom_ranges_path}")
            return 1

    metadata['short_code'] = short_code
    metadata['dataset_name'] = dataset_slug
    metadata['doc_url'] = metadata.get('doc_url') or f"{SITE_DATASET_BASE_URL}/{dataset_slug}/"
    metadata['doc_path'] = metadata.get('doc_path') or f"docs/datasets/{dataset_slug}.md"
    metadata['validation_doc_url'] = metadata.get('validation_doc_url') or f"{SITE_DATASET_BASE_URL}/{dataset_slug}/#validation"
    metadata['validation_doc_path'] = metadata.get('validation_doc_path') or f"docs/datasets/{dataset_slug}.md#validation"
    metadata['last_dataset_path'] = _relative_path(dataset_path)
    if metadata.get('validation_doc_path', '').endswith('_validation.md'):
        metadata['validation_doc_path'] = f"docs/datasets/{dataset_slug}.md#validation"
    if metadata.get('validation_doc_url', '').endswith('_validation/'):
        metadata['validation_doc_url'] = f"{SITE_DATASET_BASE_URL}/{dataset_slug}/#validation"

    # Load data ONCE and reuse throughout
    try:
        locomotion_data = _load_locomotion_data(dataset_path, "updating tasks and subjects")
    except Exception as exc:
        print(f"âŒ Unable to load dataset: {exc}")
        return 1

    detected_tasks, task_rows = _extract_task_catalog(locomotion_data)
    if detected_tasks:
        metadata['tasks'] = detected_tasks
    else:
        metadata['tasks'] = metadata.get('tasks', [])
    metadata['subjects'] = str(len(locomotion_data.get_subjects()))
    metadata['task_table'] = _build_task_table(task_rows)

    # Compute feature coverage (reuse loaded data)
    coverage_groups, coverage_tasks, coverage_source_path = _compute_feature_task_groups(
        dataset_path, metadata.get('tasks', []), locomotion_data=locomotion_data
    )
    if coverage_groups:
        metadata['feature_task_groups'] = coverage_groups
        if coverage_tasks:
            metadata['feature_task_order'] = coverage_tasks
        if coverage_source_path:
            metadata['feature_task_source'] = _relative_path(coverage_source_path)
    _free_memory()  # Clean up after feature coverage

    # Run validation (reuse loaded data)
    print(f"ðŸ” Updating validation for {dataset_slug}...")
    _, validation_summary, validation_stats, validation_ranges = run_validation(
        dataset_path, custom_ranges_path, locomotion_data=locomotion_data
    )
    _free_memory()  # Clean up after validation
    metadata['validation_summary'] = validation_summary

    ranges_display = _display_path(custom_ranges_path) if custom_ranges_path else _display_path(_resolve_default_ranges_file())
    plot_ranges_path: Optional[Path] = custom_ranges_path

    if validation_stats:
        metadata['validation_status'] = validation_stats.get('status')
        metadata['validation_pass_rate'] = validation_stats.get('pass_rate')
        metadata['validation_total_strides'] = validation_stats.get('total_strides')
        metadata['validation_passing_strides'] = validation_stats.get('passing_strides')
        if validation_stats.get('ranges_display'):
            ranges_display = validation_stats['ranges_display']
        ranges_path_str = validation_stats.get('ranges_path')
        if ranges_path_str:
            resolved = Path(ranges_path_str)
            if not resolved.is_absolute():
                resolved = (repo_root / resolved).resolve()
            plot_ranges_path = resolved
        status_text = validation_stats.get('status', '')
        pass_rate = validation_stats.get('pass_rate') or 0
        if 'PASSED' in status_text:
            metadata['quality_display'] = 'âœ… Validated'
        elif 'PARTIAL' in status_text:
            metadata['quality_display'] = f"âš ï¸ Partial ({pass_rate:.1f}%)"
        elif 'NEEDS REVIEW' in status_text:
            metadata['quality_display'] = f"âŒ Needs Review ({pass_rate:.1f}%)"
        else:
            metadata['quality_display'] = status_text or 'â€”'
        metadata['comparison_tasks'] = validation_stats.get('tasks')
    else:
        metadata['validation_status'] = 'UNKNOWN'
        metadata['validation_pass_rate'] = None
        metadata['validation_total_strides'] = None
        metadata['validation_passing_strides'] = None
        metadata['quality_display'] = 'âš ï¸ Validation Pending'
        metadata['comparison_tasks'] = metadata.get('comparison_tasks') or {}
    metadata['validation_ranges_file'] = ranges_display

    if plot_ranges_path is None:
        plot_ranges_path = _resolve_default_ranges_file()

    ranges_source_path = Path(plot_ranges_path)
    if not ranges_source_path.is_absolute():
        ranges_source_path = (repo_root / ranges_source_path).resolve()

    validation_doc_filename = f".generated/{dataset_slug}_validation.md"
    validation_doc_path = DOCS_DATASETS_DIR / validation_doc_filename

    ranges_download_filename, validation_summary = _snapshot_validation_ranges(
        dataset_slug,
        DOCS_DATASETS_DIR,
        ranges_source_path,
        ranges_display,
        validation_summary,
        metadata,
    )
    metadata['validation_summary'] = validation_summary
    ranges_source_display = metadata.get('validation_ranges_source')
    metadata['tasks'] = metadata.get('tasks', [])
    metadata['task_table'] = metadata.get('task_table', '')

    print(f"\nðŸ“Š Validation Results:")
    print(validation_summary)

    # Handle Dropbox upload (before generating docs so URLs are included)
    _handle_dropbox_upload(dataset_path, short_code, args, metadata)

    print(f"\nðŸ“„ Updating documentation...")
    doc_path, doc_body_path = generate_dataset_page(dataset_path, metadata, validation_doc_filename)
    validation_body_path = generate_validation_page(
        dataset_path,
        metadata,
        validation_summary,
        validation_stats,
        validation_ranges,
        validation_doc_path,
        ranges_download_filename,
        ranges_source_display,
    )
    print(f"âœ… Tab wrapper updated: {doc_path.relative_to(repo_root)}")
    print(f"âœ… Documentation body updated: {doc_body_path.relative_to(repo_root)}")
    print(f"âœ… Validation body updated: {validation_body_path.relative_to(repo_root)}")

    plots_dir = repo_root / "docs" / "datasets" / "validation_plots" / dataset_slug
    plots_dir.mkdir(parents=True, exist_ok=True)

    # Free memory before spawning plot subprocess
    del locomotion_data
    _free_memory("Freeing memory before plot generation...")

    try:
        plot_ranges_for_cmd = ranges_source_path if ranges_source_path.exists() else None
        _generate_validation_plots(dataset_path, plots_dir, plot_ranges_for_cmd)
    except subprocess.CalledProcessError as exc:
        print(f"âš ï¸  Plot generation failed: {exc}")

    update_validation_gallery(validation_body_path, dataset_slug)

    write_metadata_file(metadata)
    update_dataset_tables()
    generate_comparison_snapshot()

    print("âœ… Validation assets refreshed")
    return 0


def handle_remove_dataset(args):
    """Remove generated assets for a dataset."""
    short_code = args.short_code.upper()
    if not re.match(r'^[A-Z]{2}\d{2}[A-Z]?$', short_code):
        print("âŒ Short code must be 2 letters + 2 digits (optional trailing letter)")
        return 1

    dataset_slug = _slugify_short_code(short_code)
    removed_paths = _remove_dataset_for_slug(dataset_slug, include_parquet=args.remove_parquet)
    if args.remove_parquet and removed_paths:
        print("âš ï¸ Converted parquet files were deleted. Re-run your conversion script before add-dataset.")
    return 0

def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Manage dataset documentation with validation and reset workflows",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This tool helps contributors prepare complete dataset submissions.

Example workflow:
  1. Convert your data to phase-normalized parquet format
  2. Run quick validation to check data quality
  3. Use this tool to add the dataset package:
     
     python contributor_tools/manage_dataset_documentation.py add-dataset \\
         --dataset converted_datasets/your_dataset_phase.parquet
  
  4. Follow the generated checklist to complete your PR
  
  Need to refresh the overview page after editing metadata? Run:
      python contributor_tools/manage_dataset_documentation.py update-documentation \\
          --short-code YOUR_CODE [--dataset converted_datasets/your_dataset_phase.parquet]
  
  Need to refresh validation results or plots? Run:
      python contributor_tools/manage_dataset_documentation.py update-validation \\
          --short-code YOUR_CODE [--dataset converted_datasets/your_dataset_phase.parquet]
  
  Need to clean everything up? Run:
      python contributor_tools/manage_dataset_documentation.py remove-dataset \\
          --short-code YOUR_CODE [--remove-parquet]
  
  The tool will:
  - Collect metadata interactively or via --metadata-file
  - Run validation and show results when requested  
  - Generate standardized documentation
  - Create submission checklist
  - Prepare everything for PR submission
        """
    )
    
    # Create subcommand
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Add-dataset command (primary contributor workflow)
    add_parser = subparsers.add_parser(
        'add-dataset',
        help='Add or refresh documentation and submission assets for a dataset'
    )
    add_parser.add_argument(
        '--dataset',
        required=True,
        help='Path to phase-normalized dataset parquet file'
    )
    add_parser.add_argument(
        '--metadata-file',
        help='Optional YAML/JSON metadata file to run non-interactively'
    )
    add_parser.add_argument(
        '--ranges-file',
        help='Optional validation ranges YAML to override defaults'
    )
    add_parser.add_argument(
        '--short-code',
        help='Optional explicit short code (otherwise derived during prompts)'
    )
    # Dropbox integration arguments
    add_parser.add_argument(
        '--dropbox-upload',
        action='store_true',
        help='Copy dataset files to Dropbox folder'
    )
    add_parser.add_argument(
        '--dropbox-folder',
        help=f'Dropbox folder path (or set {DROPBOX_FOLDER_ENV_VAR} env var)'
    )
    add_parser.add_argument(
        '--dropbox-share',
        action='store_true',
        help=f'Generate share links via Dropbox API (requires {DROPBOX_TOKEN_ENV_VAR} env var)'
    )
    add_parser.add_argument(
        '--dropbox-domain',
        help='Restrict share links to domain (e.g., umich.edu) - requires Dropbox Business'
    )

    update_doc_parser = subparsers.add_parser(
        'update-documentation',
        help='Refresh dataset overview markdown and metadata for an existing short code'
    )
    update_doc_parser.add_argument(
        '--short-code',
        required=True,
        help='Dataset short code (e.g., UM21)'
    )
    update_doc_parser.add_argument(
        '--dataset',
        help='Path to dataset parquet; defaults to last_dataset_path from metadata'
    )
    update_doc_parser.add_argument(
        '--metadata-file',
        help='Optional metadata overrides to merge before regenerating the page'
    )
    # Dropbox integration arguments
    update_doc_parser.add_argument(
        '--dropbox-upload',
        action='store_true',
        help='Copy dataset files to Dropbox folder'
    )
    update_doc_parser.add_argument(
        '--dropbox-folder',
        help=f'Dropbox folder path (or set {DROPBOX_FOLDER_ENV_VAR} env var)'
    )
    update_doc_parser.add_argument(
        '--dropbox-share',
        action='store_true',
        help=f'Generate share links via Dropbox API (requires {DROPBOX_TOKEN_ENV_VAR} env var)'
    )
    update_doc_parser.add_argument(
        '--dropbox-domain',
        help='Restrict share links to domain (e.g., umich.edu) - requires Dropbox Business'
    )

    update_val_parser = subparsers.add_parser(
        'update-validation',
        help='Re-run validation, plots, and snapshot ranges for an existing short code'
    )
    update_val_parser.add_argument(
        '--short-code',
        required=True,
        help='Dataset short code (e.g., UM21)'
    )
    update_val_parser.add_argument(
        '--dataset',
        help='Path to dataset parquet; defaults to last_dataset_path from metadata'
    )
    update_val_parser.add_argument(
        '--ranges-file',
        help='Optional validation ranges YAML to override defaults'
    )
    # Dropbox integration arguments
    update_val_parser.add_argument(
        '--dropbox-upload',
        action='store_true',
        help='Copy dataset files to Dropbox folder'
    )
    update_val_parser.add_argument(
        '--dropbox-folder',
        help=f'Dropbox folder path (or set {DROPBOX_FOLDER_ENV_VAR} env var)'
    )
    update_val_parser.add_argument(
        '--dropbox-share',
        action='store_true',
        help=f'Generate share links via Dropbox API (requires {DROPBOX_TOKEN_ENV_VAR} env var)'
    )
    update_val_parser.add_argument(
        '--dropbox-domain',
        help='Restrict share links to domain (e.g., umich.edu) - requires Dropbox Business'
    )

    remove_parser = subparsers.add_parser(
        'remove-dataset',
        help='Delete generated docs/metadata/plots so a dataset can be rebuilt'
    )
    remove_parser.add_argument(
        '--short-code',
        required=True,
        help='Dataset short code to remove'
    )
    remove_parser.add_argument(
        '--remove-parquet',
        action='store_true',
        help='Also delete converted_datasets/*.parquet files for the slug'
    )

    refresh_parser = subparsers.add_parser(
        'refresh-tables',
        help='Regenerate dataset tables in README and docs without touching metadata'
    )
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    print(f"ðŸš€ Manage Dataset Documentation Tool")
    print(f"{'='*60}")
    
    if args.command == 'add-dataset':
        return handle_add_dataset(args)
    if args.command == 'update-documentation':
        return handle_update_documentation(args)
    if args.command == 'update-validation':
        return handle_update_validation(args)
    if args.command == 'remove-dataset':
        return handle_remove_dataset(args)
    if args.command == 'refresh-tables':
        return handle_refresh_tables(args)
    print(f"âŒ Unknown command: {args.command}")
    return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nðŸ›‘ Submission preparation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
