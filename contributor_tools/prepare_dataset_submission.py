#!/usr/bin/env python3
"""
Prepare Dataset Submission

A tool for contributors to prepare complete dataset submissions including
documentation, validation reports, and metadata.

This tool generates all necessary documentation for your dataset submission,
ensuring it's ready for maintainer review.

Usage:
    python contributor_tools/prepare_dataset_submission.py generate \
        --dataset converted_datasets/your_dataset_phase.parquet
        
The tool will:
1. Prompt for dataset metadata interactively
2. Run validation and generate reports
3. Create standardized documentation
4. Package everything for PR submission
5. Show a checklist of files to include
"""

import sys
import argparse
import yaml
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Tuple, List
import os

# Add parent directories to path for imports
current_dir = Path(__file__).parent
repo_root = current_dir.parent
sys.path.insert(0, str(repo_root))

# Import validation modules
try:
    from internal.validation_engine.report_generator import ValidationReportGenerator
except ImportError as e:
    print(f"âŒ Error importing validation modules: {e}")
    print("Make sure you're running from the repository root directory")
    sys.exit(1)


def check_existing_short_codes() -> Dict[str, str]:
    """
    Check for existing short codes in documentation.
    
    Returns:
        Dictionary mapping short codes to dataset names
    """
    codes = {}
    docs_dir = repo_root / "docs" / "datasets"
    
    if docs_dir.exists():
        for doc_file in docs_dir.glob("*.md"):
            try:
                with open(doc_file, 'r') as f:
                    content = f.read()
                    # Look for short code in frontmatter or metadata
                    match = re.search(r'short_code:\s*([A-Z0-9]+)', content)
                    if match:
                        codes[match.group(1)] = doc_file.stem
            except Exception:
                continue
    
    return codes


def _relative_path(path: Path) -> str:
    """Return repo-relative path if possible, else name."""
    try:
        return str(path.relative_to(repo_root))
    except ValueError:
        return path.name


def generate_documentation(dataset_path: Path, metadata: Dict) -> Path:
    """
    Generate dataset documentation from template.
    
    Args:
        dataset_path: Path to the dataset parquet file
        metadata: Dataset metadata dictionary
        
    Returns:
        Path to generated documentation file
    """
    # Create documentation content
    dataset_rel = _relative_path(dataset_path)

    doc_content = f"""---
title: {metadata['display_name']}
short_code: {metadata['short_code']}
date_added: {datetime.now().strftime('%Y-%m-%d')}
---

# {metadata['display_name']}

## Overview

**Short Code**: {metadata['short_code']}  
**Year**: {metadata['year']}  
**Institution**: {metadata['institution']}  

{metadata['description']}

## Dataset Information

### Subjects and Tasks
- **Number of Subjects**: {metadata['subjects']}
- **Tasks Included**: {', '.join(metadata['tasks'])}

### Data Structure
- **Format**: Phase-normalized (150 points per gait cycle)
- **Sampling**: Phase-indexed from 0-100%
- **Variables**: Standard biomechanical naming convention

## Data Access

### Download
{metadata.get('download_url', 'Contact the authors for data access.')}

### Citation
{metadata.get('citation', 'Please cite appropriately when using this dataset.')}

## Validation Results

{metadata.get('validation_summary', '*Validation results will be added here*')}

## Collection Details

### Protocol
{metadata.get('protocol', 'Standard motion capture protocol was used.')}

### Processing Notes
{metadata.get('notes', 'No additional notes.')}

## Files Included

- `{dataset_rel}` - Phase-normalized dataset
- [Validation plots](./validation_plots/{metadata['dataset_name']}/index.md) - Directory for plots
- Conversion script in `contributor_tools/conversion_scripts/{metadata['dataset_name']}/`

---

*Generated by Dataset Submission Tool on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""
    
    # Save documentation
    dataset_name = metadata['dataset_name']
    doc_dir = repo_root / "docs" / "datasets"
    doc_dir.mkdir(parents=True, exist_ok=True)
    
    doc_path = doc_dir / f"{dataset_name}.md"
    with open(doc_path, 'w') as f:
        f.write(doc_content)
    
    return doc_path


def run_validation(dataset_path: Path) -> Tuple[Dict, str]:
    """
    Run validation on the dataset.
    
    Args:
        dataset_path: Path to the dataset parquet file
        
    Returns:
        Tuple of (validation_result, summary_text)
    """
    print(f"ğŸ” Running validation...")
    
    ranges_file = repo_root / "contributor_tools" / "validation_ranges" / "default_ranges.yaml"
    if not ranges_file.exists():
        print(f"âš ï¸  Default validation ranges not found, skipping validation")
        return {}, "Validation not run (ranges file missing)"
    
    try:
        report_generator = ValidationReportGenerator(ranges_file=str(ranges_file))
        validation_result = report_generator.validator.validate(str(dataset_path))

        summary_data = validation_result.get('summary') or {}
        if not summary_data:
            dataset_hint = _relative_path(dataset_path)
            fallback = (
                "âš ï¸ Validation summary unavailable from automated run.\n"
                f"Run `python contributor_tools/quick_validation_check.py {dataset_hint}` "
                "and paste the results here."
            )
            return validation_result, fallback

        # Calculate pass rate
        total = summary_data.get('total_strides', 0)
        passed = summary_data.get('passing_strides', 0)
        pass_rate = (passed / total * 100) if total > 0 else 0
        
        # Generate summary text
        status = "âœ… PASSED" if pass_rate >= 95 else "âš ï¸ PARTIAL" if pass_rate >= 80 else "âŒ NEEDS REVIEW"
        
        summary = f"""### Summary

**Status**: {status} ({pass_rate:.1f}% valid)  
**Total Strides**: {total}  
**Passing Strides**: {passed}  

### Task Breakdown

| Task | Pass Rate | Status |
|------|-----------|--------|
"""
        
        for task, task_result in validation_result.get('tasks', {}).items():
            task_pass = task_result.get('pass_percentage', 0)
            task_status = "âœ…" if task_pass >= 90 else "âš ï¸" if task_pass >= 70 else "âŒ"
            summary += f"| {task} | {task_pass:.1f}% | {task_status} |\n"
        
        if pass_rate < 80:
            summary += "\nâš ï¸ **Note**: Low pass rates may indicate special populations or non-standard protocols. "
            summary += "Consider documenting these differences or creating custom validation ranges.\n"
        
        return validation_result, summary
        
    except Exception as e:
        print(f"âš ï¸  Validation failed: {e}")
        return {}, f"âš ï¸ Validation could not be completed: {str(e)}"


def generate_submission_checklist(dataset_name: str, doc_path: Path) -> str:
    """
    Generate a checklist for the PR submission.
    
    Args:
        dataset_name: Name of the dataset
        doc_path: Path to documentation file
        
    Returns:
        Formatted checklist string
    """
    checklist = f"""
ğŸ“‹ SUBMISSION CHECKLIST
{'='*60}

Your dataset submission is ready! Please include the following in your PR:

REQUIRED FILES:
â–¡ Dataset file: converted_datasets/{dataset_name}_phase.parquet
â–¡ Documentation: {doc_path.relative_to(repo_root)}
â–¡ Conversion script: contributor_tools/conversion_scripts/{dataset_name}/

OPTIONAL FILES:
â–¡ Validation plots: docs/datasets/validation_plots/{dataset_name}/
â–¡ Custom validation ranges (if applicable)
â–¡ README with additional details

PR DESCRIPTION TEMPLATE:
------------------------
## New Dataset: {dataset_name}

### Summary
[Brief description of the dataset]

### Validation Results
[Copy validation summary from above]

### Files Included
- Dataset parquet file
- Documentation
- Conversion script
- [Any additional files]

### Testing
- [ ] Validation passes with >80% rate
- [ ] Documentation is complete
- [ ] Conversion script is reproducible

### Notes
[Any special considerations or deviations from standard]
------------------------

NEXT STEPS:
1. Create a new branch for your changes
2. Add all files listed above
3. Commit with descriptive message
4. Push and create PR
5. Tag maintainers for review

Need help? Check the contributor guide or ask in discussions!
"""
    return checklist


def handle_generate(args):
    """Handle the generate command."""
    dataset_path = Path(args.dataset)
    
    # Validate dataset file
    if not dataset_path.exists():
        print(f"âŒ Dataset file not found: {dataset_path}")
        return 1
    
    if not dataset_path.suffix == '.parquet':
        print(f"âŒ Dataset must be a parquet file, got: {dataset_path.suffix}")
        return 1
    
    # Extract dataset name
    dataset_name = dataset_path.stem.replace("_phase", "").replace("_time", "")
    display_name = dataset_name.replace("_", " ").title()
    
    print(f"ğŸ“¦ Preparing submission for: {display_name}")
    print(f"{'='*60}")
    
    # Check for existing documentation
    doc_path = repo_root / "docs" / "datasets" / f"{dataset_name}.md"
    if doc_path.exists():
        print(f"âš ï¸  Documentation already exists: {doc_path}")
        overwrite = input("Overwrite? [y/N]: ").strip().lower()
        if overwrite != 'y':
            print("ğŸ›‘ Cancelled")
            return 0
    
    print(f"\nğŸ“ Dataset Metadata")
    print(f"{'='*40}")
    print("Please provide information about your dataset:\n")
    
    # Interactive metadata collection
    try:
        # Basic metadata
        display_name = input(f"Display name [{display_name}]: ").strip() or display_name
        
        # Short code validation
        existing_codes = check_existing_short_codes()
        while True:
            suggested_code = f"{display_name[:2].upper()}{str(datetime.now().year)[2:]}"
            short_code = input(f"Short code (2 letters + 2 digits) [{suggested_code}]: ").strip().upper()
            short_code = short_code or suggested_code
            
            if not re.match(r'^[A-Z]{2}\d{2}$', short_code):
                print("âŒ Short code must be 2 letters + 2 digits (e.g., UM21)")
                continue
                
            if short_code in existing_codes:
                print(f"âŒ Short code '{short_code}' already used by {existing_codes[short_code]}")
                continue
                
            print(f"âœ… Short code '{short_code}' accepted")
            break
        
        # Dataset details
        print("\nDataset Details:")
        description = input("Brief description (1-2 sentences): ").strip()
        if not description:
            description = f"Biomechanical dataset from {display_name}"
        
        year = input(f"Collection year [{datetime.now().year}]: ").strip()
        year = year or str(datetime.now().year)
        
        institution = input("Institution/Lab name: ").strip()
        if not institution:
            institution = "[Please add institution name]"
        
        subjects = input("Number of subjects: ").strip()
        if not subjects or not subjects.isdigit():
            subjects = "[Please add subject count]"
        
        # Task selection
        print("\nTasks included (comma-separated):")
        print("  Common: level_walking, incline_walking, stair_ascent, stair_descent")
        tasks_input = input("Tasks: ").strip()
        tasks = [t.strip() for t in tasks_input.split(",")] if tasks_input else ["[Please list tasks]"]
        
        # Optional information
        print("\nOptional Information (press Enter to skip):")
        download_url = input("Download URL: ").strip()
        citation = input("Citation: ").strip()
        protocol = input("Collection protocol notes: ").strip()
        notes = input("Additional notes: ").strip()
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Submission preparation cancelled")
        return 1
    
    # Prepare metadata dictionary
    metadata = {
        'dataset_name': dataset_name,
        'display_name': display_name,
        'short_code': short_code,
        'description': description,
        'year': year,
        'institution': institution,
        'subjects': subjects,
        'tasks': tasks,
        'download_url': download_url if download_url else None,
        'citation': citation if citation else None,
        'protocol': protocol if protocol else None,
        'notes': notes if notes else None,
    }
    
    # Run validation
    print(f"\nğŸ” Validating dataset...")
    validation_result, validation_summary = run_validation(dataset_path)
    metadata['validation_summary'] = validation_summary
    
    # Show validation results
    print(f"\nğŸ“Š Validation Results:")
    print(validation_summary)
    
    # Generate documentation
    print(f"\nğŸ“„ Generating documentation...")
    doc_path = generate_documentation(dataset_path, metadata)
    print(f"âœ… Documentation created: {doc_path.relative_to(repo_root)}")
    
    # Generate plots directory structure
    plots_dir = repo_root / "docs" / "datasets" / "validation_plots" / dataset_name
    plots_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Plot directory created: {plots_dir.relative_to(repo_root)}/")

    plots_index = plots_dir / "index.md"
    if not plots_index.exists():
        plots_rel = _relative_path(plots_dir)
        dataset_hint = _relative_path(dataset_path)
        plots_index.write_text(
            "---\n"
            f"title: {metadata['display_name']} Validation Plots\n"
            "---\n\n"
            f"# Validation Plots â€” {metadata['display_name']}\n\n"
            "Generate visual validation outputs with:\n\n"
            "```bash\n"
            "python contributor_tools/quick_validation_check.py "
            f"{dataset_hint} --plot --output-dir {plots_rel}\n"
            "```\n\n"
            "When plots are saved in this folder they will be embedded below automatically.\n"
        )

    
    # Show submission checklist
    checklist = generate_submission_checklist(dataset_name, doc_path)
    print(checklist)
    
    # Save checklist to file
    checklist_path = repo_root / f"submission_checklist_{dataset_name}.txt"
    with open(checklist_path, 'w') as f:
        f.write(checklist)
    print(f"ğŸ’¾ Checklist saved to: {checklist_path.name}")
    
    print(f"\nğŸ‰ SUCCESS! Your dataset submission is prepared!")
    print(f"   Follow the checklist above to complete your PR")
    
    return 0


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Prepare dataset submission with documentation and validation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This tool helps contributors prepare complete dataset submissions.

Example workflow:
  1. Convert your data to phase-normalized parquet format
  2. Run quick validation to check data quality
  3. Use this tool to generate documentation:
     
     python contributor_tools/prepare_dataset_submission.py generate \\
         --dataset converted_datasets/your_dataset_phase.parquet
  
  4. Follow the generated checklist to complete your PR
  
The tool will:
  - Prompt for dataset metadata interactively
  - Run validation and show results  
  - Generate standardized documentation
  - Create submission checklist
  - Prepare everything for PR submission
        """
    )
    
    # Create subcommand
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Generate command (the only command for contributors)
    generate_parser = subparsers.add_parser(
        'generate',
        help='Generate documentation and prepare submission'
    )
    generate_parser.add_argument(
        '--dataset',
        required=True,
        help='Path to phase-normalized dataset parquet file'
    )
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    print(f"ğŸš€ Dataset Submission Preparation Tool")
    print(f"{'='*60}")
    
    if args.command == 'generate':
        return handle_generate(args)
    else:
        print(f"âŒ Unknown command: {args.command}")
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Submission preparation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
